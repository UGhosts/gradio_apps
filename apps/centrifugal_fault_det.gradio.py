import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
import gradio as gr
import sys
import logging
from pathlib import Path
import io
from PIL import Image
import paddle
import paddle.nn as nn
import paddle.nn.functional as F
from utils.app_utils import AppUtils as util

warnings.filterwarnings('ignore')
plt = util.auto_config_chinese_font()

# ============================================================
# é…ç½®è·¯å¾„ (è¯·ç¡®ä¿ä¸è®­ç»ƒè„šæœ¬ä¸€è‡´)
# ============================================================
BASE_DIR = Path(__file__).parent.parent
MODEL_PATH = BASE_DIR  / "model" / "centrifugal_fault_det"/"model" / "final_model.pdparams" 
PARAMS_PATH = BASE_DIR  / "model" / "centrifugal_fault_det"/"model" / "X_train_params.npz"
DATA_DIR = BASE_DIR / "model" / "centrifugal_fault_det"/ "model" / "centrifugal_fault_det"
EXAMPLE_DIR = BASE_DIR /"dataset"/"centrifugal_fault_det"

# ============================================================
# æ•…éšœå®šä¹‰
# ============================================================
fault_code_map = {
    0: 'æ­£å¸¸çŠ¶æ€',
    1: 'æµé‡è¾“é€é˜€å¡æ¶©',
    2: 'ç¦»å¿ƒæ³µå…¥å£å µå¡',
    3: 'ç¦»å¿ƒæ³µå…¥å£æ¸©åº¦å‡é«˜æ±½èš€',
    4: 'ç¦»å¿ƒæ³µæ°”ç¼š',
    5: 'ç¦»å¿ƒæ³µå¸å…¥ç½å‹åŠ›æ§åˆ¶å…¥å£é˜€å¡æ¶©'
}

fault_analysis = {
    0: {
        'name': 'æ­£å¸¸çŠ¶æ€',
        'description': 'è®¾å¤‡è¿è¡Œæ­£å¸¸ï¼Œæ‰€æœ‰å‚æ•°åœ¨æ­£å¸¸èŒƒå›´å†…',
        'key_indicators': [
            'P101å‡ºæ–™æµé‡: ~20000 Kg/h',
            'V101å‹åŠ›: ~0.5 MPaG',
            'P101A/Bå…¥å£å‹åŠ›: ~0.475-0.5 MPaG',
            'P101A/Bå‡ºå£å‹åŠ›: ~1.5 MPaG',
            'V101æ¶²ä½: ~50%'
        ],
        'suggestions': ['ç»§ç»­ä¿æŒæ­£å¸¸è¿è¡Œ', 'å®šæœŸç»´æŠ¤ä¿å…»']
    },
    1: {
        'name': 'æµé‡è¾“é€é˜€å¡æ¶©',
        'description': 'æµé‡æ§åˆ¶é˜€é—¨å¼€åº¦å¼‚å¸¸ï¼Œå½±å“æµä½“è¾“é€',
        'key_indicators': [
            'FV101.OPï¼ˆæµé‡æ§åˆ¶é˜€å¼€åº¦ï¼‰æ³¢åŠ¨å¼‚å¸¸',
            'FT101.PVï¼ˆå‡ºæ–™æµé‡ï¼‰ä¸‹é™',
            'PI102/PI104ï¼ˆæ³µå‡ºå£å‹åŠ›ï¼‰æ³¢åŠ¨',
            'é˜€é—¨å“åº”è¿Ÿç¼“æˆ–ä¸å“åº”'
        ],
        'root_causes': [
            'é˜€èŠ¯æ±¡å¢ç§¯èš',
            'é˜€æ†å¯†å°åœˆè€åŒ–',
            'æ‰§è¡Œæœºæ„æ•…éšœ',
            'é˜€ä½“å†…éƒ¨è…èš€'
        ],
        'suggestions': [
            'æ£€æŸ¥é˜€é—¨æ‰§è¡Œæœºæ„ï¼Œç¡®è®¤æ°”æºæˆ–ç”µæºä¾›åº”æ­£å¸¸',
            'æ‹†æ£€é˜€é—¨ï¼Œæ¸…ç†é˜€èŠ¯å’Œé˜€åº§ä¸Šçš„æ±¡å¢',
            'æ£€æŸ¥é˜€æ†å¡«æ–™å’Œå¯†å°åœˆï¼Œå¿…è¦æ—¶æ›´æ¢',
            'æ¶¦æ»‘é˜€æ†ï¼Œç¡®ä¿åŠ¨ä½œçµæ´»',
            'æ ¡éªŒé˜€é—¨å®šä½å™¨ï¼Œç¡®ä¿åé¦ˆä¿¡å·å‡†ç¡®'
        ]
    },
    2: {
        'name': 'ç¦»å¿ƒæ³µå…¥å£å µå¡',
        'description': 'æ³µå…¥å£ç®¡é“æˆ–è¿‡æ»¤å™¨å µå¡ï¼Œå¯¼è‡´å¸å…¥ä¸ç•…',
        'key_indicators': [
            'PI101/PI103ï¼ˆæ³µå…¥å£å‹åŠ›ï¼‰æ˜æ˜¾é™ä½',
            'FT101.PVï¼ˆå‡ºæ–™æµé‡ï¼‰ä¸‹é™',
            'PI102/PI104ï¼ˆæ³µå‡ºå£å‹åŠ›ï¼‰ä¸‹é™',
            'æ³µæŒ¯åŠ¨å¢åŠ ï¼Œå‡ºç°å¼‚å¸¸å™ªéŸ³'
        ],
        'root_causes': [
            'è¿‡æ»¤å™¨å µå¡',
            'ç®¡é“å†…æ‚è´¨ç§¯èš',
            'å¸å…¥å£æ»¤ç½‘å µå¡',
            'ç®¡é“ç»“å¢'
        ],
        'suggestions': [
            'ç«‹å³æ£€æŸ¥æ³µå…¥å£è¿‡æ»¤å™¨ï¼Œæ¸…ç†æˆ–æ›´æ¢æ»¤èŠ¯',
            'æ£€æŸ¥å…¥å£ç®¡é“ï¼Œæ’é™¤å¼‚ç‰©å µå¡',
            'æ¸…æ´—å¸å…¥å£æ»¤ç½‘',
            'æ£€æŸ¥V101ç½åº•æ˜¯å¦æœ‰æ²‰æ·€ç‰©ï¼Œå¿…è¦æ—¶è¿›è¡Œæ¸…ç½',
            'å¢åŠ è¿‡æ»¤å™¨æ¸…æ´—é¢‘æ¬¡ï¼Œé˜²æ­¢å†æ¬¡å µå¡'
        ]
    },
    3: {
        'name': 'ç¦»å¿ƒæ³µå…¥å£æ¸©åº¦å‡é«˜æ±½èš€',
        'description': 'å…¥å£æ¸©åº¦è¿‡é«˜å¯¼è‡´æ¶²ä½“æ±½åŒ–ï¼Œäº§ç”Ÿæ±½èš€ç°è±¡',
        'key_indicators': [
            'TI101.PVï¼ˆV101è¿›æ–™æ¸©åº¦ï¼‰å‡é«˜',
            'PI101/PI103ï¼ˆæ³µå…¥å£å‹åŠ›ï¼‰æ¥è¿‘æˆ–ä½äºé¥±å’Œè’¸æ±½å‹',
            'FT101.PVï¼ˆå‡ºæ–™æµé‡ï¼‰æ³¢åŠ¨æˆ–ä¸‹é™',
            'æ³µæŒ¯åŠ¨åŠ å‰§ï¼Œå‘å‡ºæ°”æ³¡ç ´è£‚çš„å™¼å•ªå£°',
            'æ³µæ•ˆç‡æ˜¾è‘—ä¸‹é™'
        ],
        'root_causes': [
            'å†·å´ç³»ç»Ÿå¤±æ•ˆ',
            'ä¸Šæ¸¸å·¥è‰ºæ¸©åº¦æ§åˆ¶ä¸å½“',
            'V101ç½ä½“ä¿æ¸©å¤±æ•ˆ',
            'ç¯å¢ƒæ¸©åº¦è¿‡é«˜',
            'æ³µå…¥å£å‹åŠ›è¿‡ä½'
        ],
        'suggestions': [
            'ç«‹å³é™ä½V101è¿›æ–™æ¸©åº¦ï¼Œæ£€æŸ¥å†·å´ç³»ç»Ÿ',
            'æé«˜V101ç½å‹åŠ›ï¼ˆPT101.PVï¼‰ï¼Œå¢åŠ æ³µå…¥å£NPSHä½™é‡',
            'æ£€æŸ¥ä¸Šæ¸¸å·¥è‰ºï¼Œç¡®ä¿è¿›æ–™æ¸©åº¦ç¬¦åˆè®¾è®¡è¦æ±‚',
            'æ£€æŸ¥æ³µå…¥å£ç®¡é“ä¿æ¸©æƒ…å†µï¼Œå‡å°‘çƒ­é‡æŸå¤±',
            'è€ƒè™‘å¢åŠ å†·å´æ°´æµé‡æˆ–é™ä½å†·å´æ°´æ¸©åº¦',
            'å¿…è¦æ—¶é™ä½æ³µè¿è¡Œè½¬é€Ÿæˆ–æµé‡ï¼Œå‡è½»æ±½èš€ç¨‹åº¦'
        ]
    },
    4: {
        'name': 'ç¦»å¿ƒæ³µæ°”ç¼š',
        'description': 'æ³µå†…ç§¯èšæ°”ä½“ï¼Œå¯¼è‡´æ³µæ— æ³•æ­£å¸¸è¾“é€æ¶²ä½“',
        'key_indicators': [
            'FT101.PVï¼ˆå‡ºæ–™æµé‡ï¼‰éª¤é™è‡³æ¥è¿‘é›¶',
            'PI102/PI104ï¼ˆæ³µå‡ºå£å‹åŠ›ï¼‰æ˜¾è‘—ä¸‹é™',
            'PI101/PI103ï¼ˆæ³µå…¥å£å‹åŠ›ï¼‰æ­£å¸¸æˆ–ç•¥é«˜',
            'æ³µç”µæµä¸‹é™',
            'æ³µè¿è½¬å£°éŸ³å¼‚å¸¸ï¼Œç±»ä¼¼ç©ºè½¬'
        ],
        'root_causes': [
            'V101æ¶²ä½è¿‡ä½ï¼Œå¸å…¥å£æš´éœ²',
            'æ³µå¯åŠ¨å‰æœªæ’æ°”',
            'å…¥å£ç®¡é“æ¼æ°”',
            'æ¶²ä½“ä¸­æº¶è§£æ°”ä½“è¿‡å¤š',
            'æ³µå¯†å°å¤±æ•ˆå¯¼è‡´ç©ºæ°”å¸å…¥'
        ],
        'suggestions': [
            'ç«‹å³åœæ³µï¼Œæ£€æŸ¥V101æ¶²ä½ï¼ˆLT101.PVï¼‰ï¼Œç¡®ä¿æ¶²ä½æ­£å¸¸',
            'æ‰“å¼€æ³µé¡¶éƒ¨æ’æ°”é˜€ï¼Œå……åˆ†æ’å‡ºæ°”ä½“',
            'æ£€æŸ¥æ³µå…¥å£ç®¡é“åŠæ³•å…°è¿æ¥ï¼Œæ’é™¤æ¼æ°”ç‚¹',
            'æ£€æŸ¥æ³µæœºæ¢°å¯†å°ï¼Œç¡®ä¿å¯†å°å®Œå¥½',
            'é‡æ–°å¯åŠ¨å‰ç¡®ä¿æ³µä½“å……æ»¡æ¶²ä½“',
            'è‹¥æ¶²ä½æ­£å¸¸ä½†ä»æ°”ç¼šï¼Œæ£€æŸ¥æ¶²ä½“æ˜¯å¦å«æ°”è¿‡å¤šï¼Œå¿…è¦æ—¶å¢åŠ è„±æ°”æªæ–½'
        ]
    },
    5: {
        'name': 'ç¦»å¿ƒæ³µå¸å…¥ç½å‹åŠ›æ§åˆ¶å…¥å£é˜€å¡æ¶©',
        'description': 'V101å‹åŠ›æ§åˆ¶å…¥å£é˜€ï¼ˆPV101Aï¼‰å¡æ¶©ï¼Œå½±å“ç½å‹è°ƒèŠ‚',
        'key_indicators': [
            'PV101A.OPï¼ˆå‹åŠ›æ§åˆ¶å…¥å£é˜€å¼€åº¦ï¼‰å¼‚å¸¸æˆ–ä¸å˜åŒ–',
            'PT101.PVï¼ˆV101å‹åŠ›ï¼‰æ³¢åŠ¨æˆ–åç¦»è®¾å®šå€¼',
            'å‹åŠ›æ§åˆ¶å“åº”è¿Ÿç¼“',
            'LT101.PVï¼ˆV101æ¶²ä½ï¼‰å¯èƒ½æ³¢åŠ¨'
        ],
        'root_causes': [
            'é˜€èŠ¯å¡æ­»',
            'æ‰§è¡Œæœºæ„æ•…éšœ',
            'é˜€é—¨å®šä½å™¨å¤±æ•ˆ',
            'æ§åˆ¶ä¿¡å·å¼‚å¸¸',
            'é˜€ä½“å†…éƒ¨ç»“å¢æˆ–è…èš€'
        ],
        'suggestions': [
            'åˆ‡æ¢è‡³æ‰‹åŠ¨æ§åˆ¶ï¼Œæ‰‹åŠ¨è°ƒèŠ‚PV101Aé˜€é—¨å¼€åº¦',
            'æ£€æŸ¥é˜€é—¨æ‰§è¡Œæœºæ„å’Œå®šä½å™¨ï¼Œç¡®è®¤åŠ¨ä½œæ˜¯å¦æ­£å¸¸',
            'æ£€æŸ¥æ§åˆ¶ç³»ç»Ÿä¿¡å·ï¼Œç¡®ä¿PIDæ§åˆ¶å™¨è¾“å‡ºæ­£å¸¸',
            'å¿…è¦æ—¶æ‹†æ£€é˜€é—¨ï¼Œæ¸…ç†é˜€èŠ¯å’Œé˜€åº§',
            'æ¶¦æ»‘é˜€æ†ï¼Œç¡®ä¿é˜€é—¨åŠ¨ä½œçµæ´»',
            'æ ¡éªŒå‹åŠ›å˜é€å™¨ï¼ˆPT101ï¼‰ï¼Œç¡®ä¿æµ‹é‡å‡†ç¡®',
            'æ£€æŸ¥å‹åŠ›æ§åˆ¶é€»è¾‘ï¼Œä¼˜åŒ–PIDå‚æ•°'
        ]
    }
}

# ç‰¹å¾ä¸­æ–‡åç§°æ˜ å°„
feature_names_cn = {
    'FT101': 'P101å‡ºæ–™æµé‡',
    'PI101': 'P101Aå…¥å£å‹åŠ›',
    'PI102': 'P101Aå‡ºå£å‹åŠ›',
    'PT101': 'V101 å‹åŠ›å˜é€å™¨',
    'TT101': 'V101 è¿›æ–™æ¸©åº¦',
    'LV101': 'V101æ¶²ä½æ§åˆ¶é˜€å¼€åº¦',
    'PV101A': 'V101å‹åŠ›æ§åˆ¶è¿›å£é˜€å¼€åº¦',
    'PV101B': 'V101å‹åŠ›æ§åˆ¶å‡ºå£é˜€å¼€åº¦',
    'FV101': 'P101å‡ºå£æµé‡æ§åˆ¶é˜€å¼€åº¦'
}


# ============================================================
# æ¨¡å‹å®šä¹‰ - å¿…é¡»ä¸è®­ç»ƒæ—¶çš„ FaultNet å®Œå…¨ä¸€è‡´
# ============================================================
class FaultNet(nn.Layer):
    def __init__(self, num_classes):
        super(FaultNet, self).__init__()
        self.conv_block = nn.Sequential(
            nn.Conv1D(1, 64, kernel_size=3, padding=1),
            nn.BatchNorm1D(64),
            nn.ReLU(),
            nn.MaxPool1D(2),
            
            nn.Conv1D(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm1D(128),
            nn.ReLU(),
            nn.AdaptiveMaxPool1D(1) 
        )
        self.fc = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        x = self.conv_block(x)
        x = paddle.flatten(x, 1)
        x = self.fc(x)
        return x

# ============================================================
# æ¨ç†ç±»ä¼˜åŒ–
# ============================================================
class FaultClassifier:
    def __init__(self, model_path, params_path):
        # åŠ è½½å½’ä¸€åŒ–å‚æ•°
        self.params = np.load(params_path)
        self.mean = self.params['mean'].astype('float32')
        self.std = self.params['std'].astype('float32')
        
        num_classes = 6
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = FaultNet(num_classes)
        
        # åŠ è½½æƒé‡
        state_dict = paddle.load(str(model_path))
        self.model.set_state_dict(state_dict)
        self.model.eval()
        
        logging.info(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ¨¡å‹è·¯å¾„: {model_path}")
    
    def normalize_data(self, X):
        """å®Œå…¨å¯¹é½è®­ç»ƒæ—¶çš„å½’ä¸€åŒ–é€»è¾‘"""
        normalized = ((X - self.mean) / (self.std + 1e-8)).astype('float32')
        return normalized
    
    def predict(self, df):
        """æ‰¹é‡é¢„æµ‹"""
        # 1. ç¡®ä¿ç‰¹å¾é¡ºåº
        expected_cols = list(feature_names_cn.keys())
        X_raw = df[expected_cols].values.astype('float32')
        
        # 2. å½’ä¸€åŒ–
        X_normalized = self.normalize_data(X_raw)
        
        # 3. è°ƒæ•´å½¢çŠ¶ [Batch, Channels, Features]
        X_input = X_normalized.reshape(-1, 1, X_normalized.shape[1])
        X_tensor = paddle.to_tensor(X_input, dtype='float32')
        
        # 4. æ¨ç†
        with paddle.no_grad():
            logits = self.model(X_tensor)
            probs = F.softmax(logits, axis=1)
            preds = paddle.argmax(logits, axis=1)
        
        # 5. ç»„è£…ç»“æœ
        results = pd.DataFrame({
            'fault_code': preds.numpy(),
            'fault_name': [fault_code_map[c] for c in preds.numpy()],
            'confidence': probs.numpy().max(axis=1)
        })
        
        # æ·»åŠ è¯¦ç»†æ¦‚ç‡åˆ—
        for i in range(6):
            results[f'prob_class_{i}'] = probs.numpy()[:, i]
        
        return results

# ============================================================
# æ ·ä¾‹æ•°æ®ç®¡ç†
# ============================================================
def get_example_files():
    """è·å–æ‰€æœ‰æ ·ä¾‹æ–‡ä»¶"""
    if not EXAMPLE_DIR.exists():
        EXAMPLE_DIR.mkdir(parents=True, exist_ok=True)
        logging.warning(f"æ ·ä¾‹ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»º: {EXAMPLE_DIR}")
        return {}
    
    example_files = {}
    for file in EXAMPLE_DIR.glob("*.csv"):
        # ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºæ˜¾ç¤ºåç§°
        display_name = file.stem
        example_files[display_name] = str(file)
    
    return example_files

# ============================================================
# è¯Šæ–­é€»è¾‘ä¼˜åŒ–
# ============================================================
def diagnose_from_data(csv_file, example_choice):
    """
    ä»ä¸Šä¼ æ–‡ä»¶æˆ–æ ·ä¾‹æ•°æ®è¿›è¡Œè¯Šæ–­
    ä¼˜å…ˆä½¿ç”¨ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰ä¸Šä¼ åˆ™ä½¿ç”¨é€‰æ‹©çš„æ ·ä¾‹
    """
    global classifier
    
    # ç¡®å®šä½¿ç”¨å“ªä¸ªæ•°æ®æº
    data_source = None
    source_name = ""
    
    if csv_file is not None:
        # ä¼˜å…ˆä½¿ç”¨ä¸Šä¼ çš„æ–‡ä»¶
        data_source = csv_file.name
        source_name = Path(csv_file.name).name
    elif example_choice and example_choice != "è¯·é€‰æ‹©æ ·ä¾‹æ•°æ®":
        # ä½¿ç”¨é€‰æ‹©çš„æ ·ä¾‹
        example_files = get_example_files()
        if example_choice in example_files:
            data_source = example_files[example_choice]
            source_name = f"æ ·ä¾‹: {example_choice}"
        else:
            return None, "âŒ é€‰æ‹©çš„æ ·ä¾‹æ–‡ä»¶ä¸å­˜åœ¨", None, None
    else:
        return None, "âŒ è¯·ä¸Šä¼ CSVæ–‡ä»¶æˆ–é€‰æ‹©æ ·ä¾‹æ•°æ®ï¼", None, None
    
    # åŠ è½½åˆ†ç±»å™¨
    if classifier is None:
        status = load_classifier()
        if "å¤±è´¥" in status:
            return None, status, None, None
    
    try:
        # è¯»å–æ•°æ®
        try:
            df = pd.read_csv(data_source, encoding='utf-8')
        except:
            df = pd.read_csv(data_source, encoding='gbk')
            
        # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿ
        if len(df) < 1:
            return None, f"âŒ æ•°æ®æ–‡ä»¶ [{source_name}] æ•°æ®é‡ä¸è¶³", None, None

        # æ£€æŸ¥ç‰¹å¾åˆ—
        expected_cols = list(feature_names_cn.keys())
        df.columns = df.columns.str.strip()
        missing_cols = set(expected_cols) - set(df.columns)
        if missing_cols:
            return None, f"âŒ æ•°æ®æ–‡ä»¶ [{source_name}] ç¼ºå°‘ç‰¹å¾åˆ—: {missing_cols}", None, None
        
        # æ‰§è¡Œé¢„æµ‹
        predictions = classifier.predict(df)
        
        # åˆå¹¶ç»“æœ
        output_df = pd.concat([df.reset_index(drop=True), predictions], axis=1)
        
        # ç”Ÿæˆå¯è§†åŒ–å’ŒæŠ¥å‘Š
        report_text = generate_fault_report(predictions, source_name)
        distribution_chart = create_distribution_chart(predictions)
        confidence_chart = create_confidence_chart(predictions)
        
        # ä¿å­˜è¯Šæ–­è®°å½•
        output_path = DATA_DIR / f"diagnosis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        summary = (f"ğŸ“Š è¯Šæ–­å®Œæˆï¼\n"
                   f"æ•°æ®æº: {source_name}\n"
                   f"æ ·æœ¬æ€»æ•°: {len(df)}\n"
                   f"ä¸»è¦ç»“è®º: {predictions['fault_name'].mode()[0]}\n"
                   f"ç»“æœå·²å­˜è‡³: {output_path.name}")
        
        return summary, report_text, distribution_chart, confidence_chart
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†æ•°æ® [{source_name}] æ—¶å‡ºé”™: {str(e)}\n{traceback.format_exc()}"
        return None, error_msg, None, None

def generate_fault_report(predictions, source_name=""):
    """ç”Ÿæˆè¯¦ç»†æ•…éšœæŠ¥å‘Š"""
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("ç¦»å¿ƒæ³µæ•…éšœè¯Šæ–­åˆ†ææŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    if source_name:
        report_lines.append(f"æ•°æ®æº: {source_name}")
    report_lines.append(f"æ ·æœ¬æ€»æ•°: {len(predictions)}")
    report_lines.append("")
    
    fault_counts = predictions['fault_code'].value_counts().sort_index()
    
    report_lines.append("ã€æ•…éšœåˆ†å¸ƒç»Ÿè®¡ã€‘")
    report_lines.append("-" * 80)
    for fault_code, count in fault_counts.items():
        percentage = count / len(predictions) * 100
        fault_name = fault_code_map[fault_code]
        avg_conf = predictions[predictions['fault_code'] == fault_code]['confidence'].mean()
        report_lines.append(
            f"  [{fault_code}] {fault_name}: {count}æ¬¡ ({percentage:.2f}%) "
            f"- å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.2%}"
        )
    report_lines.append("")
    
    # è¯¦ç»†åˆ†æ
    for fault_code in fault_counts.index:
        if fault_code == 0:  # è·³è¿‡æ­£å¸¸çŠ¶æ€çš„è¯¦ç»†åˆ†æ
            continue
            
        analysis = fault_analysis[fault_code]
        count = fault_counts[fault_code]
        percentage = count / len(predictions) * 100
        
        report_lines.append("=" * 80)
        report_lines.append(f"ã€æ•…éšœç±»å‹ {fault_code}ã€‘{analysis['name']}")
        report_lines.append("=" * 80)
        report_lines.append(f"æ£€å‡ºæ¬¡æ•°: {count}æ¬¡ ({percentage:.2f}%)")
        report_lines.append(f"\næ•…éšœæè¿°: {analysis['description']}\n")
        
        report_lines.append("â–º å…³é”®æŒ‡æ ‡:")
        for indicator in analysis['key_indicators']:
            report_lines.append(f"  â€¢ {indicator}")
        report_lines.append("")
        
        if 'root_causes' in analysis:
            report_lines.append("â–º å¯èƒ½åŸå› :")
            for cause in analysis['root_causes']:
                report_lines.append(f"  â€¢ {cause}")
            report_lines.append("")
        
        if 'suggestions' in analysis:
            report_lines.append("â–º å¤„ç†å»ºè®®:")
            for i, suggestion in enumerate(analysis['suggestions'], 1):
                report_lines.append(f"  {i}. {suggestion}")
            report_lines.append("")
    
    report_lines.append("=" * 80)
    report_lines.append("æŠ¥å‘Šç»“æŸ")
    report_lines.append("=" * 80)
    
    return "\n".join(report_lines)

def create_distribution_chart(predictions):
    """åˆ›å»ºæ•…éšœåˆ†å¸ƒå›¾è¡¨"""
    fault_counts = predictions['fault_code'].value_counts().sort_index()
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # æŸ±çŠ¶å›¾
    fault_names = [fault_code_map[code] for code in fault_counts.index]
    colors = plt.cm.Set3(range(len(fault_counts)))
    
    bars = ax1.bar(range(len(fault_counts)), fault_counts.values, 
                   color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
    ax1.set_xticks(range(len(fault_counts)))
    ax1.set_xticklabels([f'{name}\n(ä»£ç {code})' 
                         for name, code in zip(fault_names, fault_counts.index)], 
                        rotation=45, ha='right', fontsize=9)
    ax1.set_ylabel('æ£€å‡ºæ¬¡æ•°', fontsize=12, fontweight='bold')
    ax1.set_title('æ•…éšœç±»å‹æ£€å‡ºæ¬¡æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax1.grid(axis='y', alpha=0.3, linestyle='--')
    
    for bar, count in zip(bars, fault_counts.values):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(count)}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # é¥¼å›¾
    wedges, texts, autotexts = ax2.pie(
        fault_counts.values, 
        labels=[f'{name}' for name in fault_names],
        autopct='%1.1f%%',
        colors=colors,
        explode=[0.05] * len(fault_counts),
        shadow=True,
        startangle=90
    )
    
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontsize(11)
        autotext.set_fontweight('bold')
    
    for text in texts:
        text.set_fontsize(10)
    
    ax2.set_title('æ•…éšœç±»å‹å æ¯”åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    
    plt.suptitle('ç¦»å¿ƒæ³µæ•…éšœè¯Šæ–­ç»“æœåˆ†å¸ƒ', fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    
    buf = io.BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight', dpi=120)
    buf.seek(0)
    img = Image.open(buf)
    plt.close(fig)
    
    return img

def create_confidence_chart(predictions):
    """åˆ›å»ºç½®ä¿¡åº¦åˆ†æå›¾è¡¨"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle('å„æ•…éšœç±»å‹é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ', fontsize=16, fontweight='bold')
    
    axes = axes.flatten()
    
    for i in range(6):
        ax = axes[i]
        fault_name = fault_code_map[i]
        prob_col = f'prob_class_{i}'
        
        data = predictions[prob_col]
        
        ax.hist(data, bins=50, color=plt.cm.Set3(i), alpha=0.7, edgecolor='black')
        ax.axvline(data.mean(), color='red', linestyle='--', linewidth=2, 
                   label=f'å‡å€¼: {data.mean():.3f}')
        ax.set_title(f'[{i}] {fault_name}', fontsize=12, fontweight='bold')
        ax.set_xlabel('é¢„æµ‹æ¦‚ç‡', fontsize=10)
        ax.set_ylabel('æ ·æœ¬æ•°é‡', fontsize=10)
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    buf = io.BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight', dpi=120)
    buf.seek(0)
    img = Image.open(buf)
    plt.close(fig)
    
    return img


def load_classifier():
    global classifier
    try:
        real_model_path = MODEL_PATH
        if not real_model_path.exists():
            alt_path = MODEL_PATH.with_suffix('')
            if alt_path.exists():
                real_model_path = alt_path
            else:
                return f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}\nè¯·ç¡®è®¤è®­ç»ƒåå·²ç”Ÿæˆ .pdparams æ–‡ä»¶"
        
        if not PARAMS_PATH.exists():
            return f"âŒ æ‰¾ä¸åˆ°å‡å€¼å‚æ•°æ–‡ä»¶: {PARAMS_PATH}"
        
        classifier = FaultClassifier(str(real_model_path), str(PARAMS_PATH))
        return "âœ… æ¨¡å‹ä¸å‚æ•°åŠ è½½æˆåŠŸï¼"
    except Exception as e:
        return f"âŒ åŠ è½½å¤±è´¥: {str(e)}"

def create_gradio_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    # è·å–æ ·ä¾‹æ–‡ä»¶åˆ—è¡¨
    example_files = get_example_files()
    example_choices = ["è¯·é€‰æ‹©æ ·ä¾‹æ•°æ®"] + list(example_files.keys())
    
    with gr.Blocks(title="ç¦»å¿ƒæ³µæ™ºèƒ½æ•…éšœè¯Šæ–­ç³»ç»Ÿ") as iface:
        with gr.Tab("ğŸ“Š æ•…éšœè¯Šæ–­"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“ æ•°æ®è¾“å…¥")
                    
                    # æ ·ä¾‹æ•°æ®é€‰æ‹©
                    gr.Markdown("#### æ–¹å¼ä¸€ï¼šé€‰æ‹©æ ·ä¾‹æ•°æ®")
                    example_dropdown = gr.Dropdown(
                        choices=example_choices,
                        value="è¯·é€‰æ‹©æ ·ä¾‹æ•°æ®",
                        label="æ ·ä¾‹æ•°æ®ï¼ˆé€‰æ‹©é¢„ç½®çš„æ ·ä¾‹æ•°æ®è¿›è¡Œåˆ†æï¼‰"
                    )
                    
                    gr.Markdown("#### æ–¹å¼äºŒï¼šä¸Šä¼ è‡ªå®šä¹‰æ•°æ®")
                    csv_input = gr.File(
                        label="ä¸Šä¼ CSVæ–‡ä»¶ï¼ˆä¸Šä¼ çš„æ–‡ä»¶å°†ä¼˜å…ˆäºæ ·ä¾‹æ•°æ®ï¼‰",
                        file_types=[".csv"],
                        type="filepath"
                    )
                    
                    gr.Markdown("---")
                    diagnose_btn = gr.Button("ğŸ” å¼€å§‹è¯Šæ–­", variant="primary", size="lg")
                    
                    # æç¤ºä¿¡æ¯
                    with gr.Accordion("ğŸ’¡ ä½¿ç”¨è¯´æ˜", open=False):
                        gr.Markdown("""
                        1. **ä½¿ç”¨æ ·ä¾‹**: ä»ä¸‹æ‹‰èœå•é€‰æ‹©æ ·ä¾‹æ•°æ®åï¼Œç‚¹å‡»"å¼€å§‹è¯Šæ–­"
                        2. **ä¸Šä¼ æ–‡ä»¶**: ä¸Šä¼ CSVæ–‡ä»¶åï¼Œç‚¹å‡»"å¼€å§‹è¯Šæ–­"ï¼ˆä¼˜å…ˆçº§é«˜äºæ ·ä¾‹ï¼‰
                        3. **æ•°æ®æ ¼å¼**: CSVéœ€åŒ…å«ä»¥ä¸‹ç‰¹å¾åˆ—ï¼š
                           - FT101, PI101, PI102, PT101, TT101
                           - LV101, PV101A, PV101B, FV101
                        """)
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“Š è¯Šæ–­ç»“æœ")
                    result_summary = gr.Textbox(label="è¯Šæ–­æ¦‚è¦", lines=10, interactive=False)
                    
                    with gr.Row():
                        distribution_chart = gr.Image(label="æ•…éšœåˆ†å¸ƒåˆ†æ", height=350,buttons=['fullscreen'])
                        confidence_chart = gr.Image(label="ç½®ä¿¡åº¦åˆ†æ", height=350,buttons=['fullscreen'])
                    
                    fault_report = gr.Textbox(label="è¯¦ç»†æ•…éšœåˆ†ææŠ¥å‘Š", lines=30, interactive=False)
        
        # äº‹ä»¶ç»‘å®š
        diagnose_btn.click(
            diagnose_from_data,
            inputs=[csv_input, example_dropdown],
            outputs=[result_summary, fault_report, distribution_chart, confidence_chart]
        )
    
    return iface

# ============================================================
# ä¸»å‡½æ•°
# ============================================================
def main():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print(f"\n{'='*80}")
    print("ç¦»å¿ƒæ³µæ™ºèƒ½æ•…éšœè¯Šæ–­ç³»ç»Ÿ")
    print(f"{'='*80}\n")
    
    # åˆå§‹åŒ–æ¨¡å‹
    status = load_classifier()
    print(status)
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    EXAMPLE_DIR.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥æ ·ä¾‹æ–‡ä»¶
    example_files = get_example_files()
    if example_files:
        print(f"\nâœ“ å‘ç° {len(example_files)} ä¸ªæ ·ä¾‹æ–‡ä»¶:")
        for name in example_files.keys():
            print(f"  - {name}")
    else:
        print(f"\nâš ï¸  æœªå‘ç°æ ·ä¾‹æ–‡ä»¶")
        print(f"   è¯·åœ¨ {EXAMPLE_DIR} ç›®å½•ä¸‹æ·»åŠ CSVæ ·ä¾‹æ–‡ä»¶")
    
    # å¯åŠ¨ç•Œé¢
    port = 7865
    if len(sys.argv) > 1:
        try:
            port = int(sys.argv[1])
        except ValueError:
            logging.warning(f"æ— æ•ˆç«¯å£å·ï¼Œä½¿ç”¨é»˜è®¤ç«¯å£ {port}")
    
    print(f"\n{'='*80}")
    print(f"å¯åŠ¨Webç•Œé¢: http://0.0.0.0:{port}")
    print(f"{'='*80}\n")
    
    iface = create_gradio_interface()
    iface.launch(
        server_name="0.0.0.0",
        server_port=port,
        share=False
    )

if __name__ == '__main__':
    main()