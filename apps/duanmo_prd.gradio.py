import random
from datetime import datetime

import gradio as gr
import time
import os
import sys
import csv
selected_preset = None
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
BASE_DIR = Path(__file__).parent.parent
from utils.app_utils import AppUtils as util
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
plt = util.auto_config_chinese_font()
from sklearn.utils import shuffle
import warnings
import joblib
import os

os.makedirs(f'{BASE_DIR}/output/duanmo_prd/', exist_ok=True)

def generate_membrane_report(excel_path, save_image_path=None):
    # -------------------------- åˆå§‹åŒ–é…ç½® --------------------------
    # å›¾ç‰‡è·¯å¾„å¤„ç†
    if save_image_path is None:
        # é»˜è®¤ä¿å­˜è·¯å¾„ï¼šå½“å‰ç›®å½• + æ—¶é—´æˆ³
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_image_path = f'æ–­è†œä¸å¼‚å¸¸æ¦‚ç‡è¶‹åŠ¿å›¾_{timestamp}.png'
    else:
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        save_dir = os.path.dirname(save_image_path)
        if save_dir and not os.path.exists(save_dir):
            os.makedirs(save_dir)

    # -------------------------- 1. è¯»å–å¹¶éªŒè¯æ•°æ® --------------------------
    try:
        df = pd.read_excel(excel_path)
    except Exception as e:
        raise ValueError(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥ï¼š{str(e)}")

    # éªŒè¯å¿…è¦åˆ—
    required_columns = ['SAVETIME', 'å»¶ä¼¸å  å¤„ç†ä¸Š/Ré€Ÿåº¦ç°åœ¨ç›‘è§†å™¨', 'å¼‚å¸¸æ¦‚ç‡']
    for col in required_columns:
        if col not in df.columns:
            raise ValueError(f"Excelæ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—ï¼š{col}")

    # æ£€æŸ¥åç¦»æ¨¡å¼åˆ—æ˜¯å¦å­˜åœ¨ï¼ˆæŠ¥å‘Šéœ€è¦ï¼‰
    if 'åç¦»æ¨¡å¼' not in df.columns:
        df['åç¦»æ¨¡å¼'] = 'æ— æ•°æ®'  # å¡«å……é»˜è®¤å€¼

    # -------------------------- 2. æ•°æ®é¢„å¤„ç† --------------------------
    # æå–éœ€è¦çš„åˆ—å¹¶åˆ é™¤ç¼ºå¤±å€¼
    df_clean = df[['SAVETIME', 'å»¶ä¼¸å  å¤„ç†ä¸Š/Ré€Ÿåº¦ç°åœ¨ç›‘è§†å™¨', 'å¼‚å¸¸æ¦‚ç‡', 'åç¦»æ¨¡å¼']].copy()
    df_clean = df_clean.dropna(subset=['SAVETIME', 'å»¶ä¼¸å  å¤„ç†ä¸Š/Ré€Ÿåº¦ç°åœ¨ç›‘è§†å™¨', 'å¼‚å¸¸æ¦‚ç‡'])

    # å¤„ç†â€œæ˜¯å¦æ–­è†œâ€åˆ—ï¼šé€Ÿåº¦ä¸º0â†’y=1ï¼Œé0â†’y=0
    df_clean['æ˜¯å¦æ–­è†œ'] = df_clean['å»¶ä¼¸å  å¤„ç†ä¸Š/Ré€Ÿåº¦ç°åœ¨ç›‘è§†å™¨'].apply(lambda x: 1 if x == 0 else 0)

    # æ—¶é—´æ’åº
    df_clean = df_clean.sort_values(by='SAVETIME').reset_index(drop=True)

    # ç­›é€‰å¼‚å¸¸æ¦‚ç‡>0.5çš„ç‚¹ï¼ˆç”¨äºæ ‡æ³¨å’ŒæŠ¥å‘Šï¼‰
    high_prob_points = df_clean[df_clean['å¼‚å¸¸æ¦‚ç‡'] > 0.5].copy()

    # -------------------------- 3. æ ¸å¿ƒå‡½æ•°å®šä¹‰ --------------------------
    def plot_continuous_color_line(ax, x, y, threshold=0.5, color_below='#00A86B', color_above='#E63946', linewidth=2):
        """ç»˜åˆ¶è¿ç»­çš„åŒè‰²çº¿æ¡"""
        intersect_points = []
        x_arr = np.array(x)
        y_arr = np.array(y)

        for i in range(len(x_arr) - 1):
            y1, y2 = y_arr[i], y_arr[i + 1]
            x1, x2 = x_arr[i], x_arr[i + 1]

            if y1 <= threshold and y2 <= threshold:
                ax.plot([x1, x2], [y1, y2], color=color_below, linewidth=linewidth, alpha=0.9)
            elif y1 > threshold and y2 > threshold:
                ax.plot([x1, x2], [y1, y2], color=color_above, linewidth=linewidth, alpha=0.9)
            else:
                slope = (y2 - y1) / (x2 - x1) if x2 != x1 else 0
                if slope == 0:
                    x_intersect = x1
                else:
                    x_intersect = x1 + (threshold - y1) / slope
                y_intersect = threshold
                intersect_points.append((x_intersect, y_intersect))

                if y1 <= threshold:
                    ax.plot([x1, x_intersect], [y1, y_intersect], color=color_below, linewidth=linewidth, alpha=0.9)
                    ax.plot([x_intersect, x2], [y_intersect, y2], color=color_above, linewidth=linewidth, alpha=0.9)
                else:
                    ax.plot([x1, x_intersect], [y1, y_intersect], color=color_above, linewidth=linewidth, alpha=0.9)
                    ax.plot([x_intersect, x2], [y_intersect, y2], color=color_below, linewidth=linewidth, alpha=0.9)
        return intersect_points

    def annotate_high_prob_points(ax, df_high, x_base, offset_y=0.05):
        """æ ‡æ³¨é«˜æ¦‚ç‡å€¼"""
        for idx, row in df_high.iterrows():
            x_pos = idx
            y_pos = row['å¼‚å¸¸æ¦‚ç‡'] + offset_y
            prob_value = round(row['å¼‚å¸¸æ¦‚ç‡'], 4)

            ax.annotate(
                f'{prob_value}',
                xy=(x_pos, row['å¼‚å¸¸æ¦‚ç‡']),
                xytext=(x_pos, y_pos),
                fontsize=8,
                color='#E63946',
                fontweight='bold',
                ha='center',
                va='bottom',
                bbox=dict(
                    boxstyle='round,pad=0.2',
                    facecolor='white',
                    edgecolor='#E63946',
                    alpha=0.8
                ),
                arrowprops=dict(
                    arrowstyle='->',
                    color='#E63946',
                    alpha=0.6,
                    lw=0.8
                )
            )

    # -------------------------- 4. ç»˜åˆ¶å›¾è¡¨ --------------------------
    fig, ax1 = plt.subplots(figsize=(18, 10))

    # é¢œè‰²æ–¹æ¡ˆ
    color_membrane = '#2E86AB'
    color_prob_green = '#00A86B'
    color_prob_red = '#E63946'
    color_threshold = '#C73E1D'

    # å·¦è½´ï¼šæ˜¯å¦æ–­è†œ
    ax1.set_xlabel('æ—¶é—´ï¼ˆSAVETIMEï¼‰', fontsize=12, fontweight='bold')
    ax1.set_ylabel('æ˜¯å¦æ–­è†œ', color=color_membrane, fontsize=12, fontweight='bold')
    ax1.step(
        range(len(df_clean)),
        df_clean['æ˜¯å¦æ–­è†œ'],
        color=color_membrane,
        linewidth=2.5,
        alpha=0.8,
        where='mid'
    )
    ax1.tick_params(axis='y', labelcolor=color_membrane, labelsize=10)
    ax1.set_ylim(-0.1, 1.1)
    ax1.set_yticks([0, 1])
    ax1.set_yticklabels(['æ­£å¸¸ï¼ˆé0ï¼‰', 'æ–­è†œï¼ˆ0ï¼‰'], fontsize=10)

    # å³è½´ï¼šå¼‚å¸¸æ¦‚ç‡
    ax2 = ax1.twinx()
    ax2.set_ylabel('å¼‚å¸¸æ¦‚ç‡', fontsize=12, fontweight='bold')

    # ç»˜åˆ¶åŒè‰²è¿ç»­çº¿
    x_data = range(len(df_clean))
    y_data = df_clean['å¼‚å¸¸æ¦‚ç‡'].values
    plot_continuous_color_line(ax2, x_data, y_data, threshold=0.5)

    # ç»˜åˆ¶é˜ˆå€¼çº¿
    ax2.axhline(
        y=0.5,
        color=color_threshold,
        linestyle='--',
        linewidth=2,
        alpha=0.7,
        label='é˜ˆå€¼=0.5'
    )

    # æ ‡æ³¨é«˜æ¦‚ç‡ç‚¹
    if len(high_prob_points) > 0:
        annotate_high_prob_points(ax2, high_prob_points, x_data)

    # å³è½´æ ·å¼
    ax2.tick_params(axis='y', labelsize=10)
    ax2.set_ylim(0, 1.15)
    ax2.set_yticks(np.arange(0, 1.1, 0.1))

    # å›¾ä¾‹
    from matplotlib.lines import Line2D
    custom_lines = [
        Line2D([0], [0], color=color_prob_green, lw=2, label='å¼‚å¸¸æ¦‚ç‡â‰¤0.5'),
        Line2D([0], [0], color=color_prob_red, lw=2, label='å¼‚å¸¸æ¦‚ç‡>0.5ï¼ˆæ ‡æ³¨æ•°å€¼ï¼‰'),
        Line2D([0], [0], color=color_threshold, lw=2, linestyle='--', label='é˜ˆå€¼=0.5')
    ]
    ax2.legend(handles=custom_lines, loc='upper right', fontsize=10, frameon=True, shadow=True)

    # æ¨ªåæ ‡ä¼˜åŒ–
    step = max(1, len(df_clean) // 25)
    x_ticks = range(0, len(df_clean), step)
    x_tick_labels = [t.strftime('%H:%M:%S') for t in df_clean['SAVETIME'].iloc[x_ticks]]
    ax1.set_xticks(x_ticks)
    ax1.set_xticklabels(x_tick_labels, rotation=45, ha='right', fontsize=9)

    # æ ‡é¢˜å’Œç½‘æ ¼
    plt.title(
        'æ–­è†œçŠ¶æ€ä¸å¼‚å¸¸æ¦‚ç‡è¶‹åŠ¿å›¾ï¼ˆæ ‡æ³¨>0.5æ¦‚ç‡å€¼ï¼‰',
        fontsize=16,
        fontweight='bold',
        pad=20
    )
    ax1.grid(True, axis='y', alpha=0.3, linestyle='-', linewidth=0.5)
    ax2.grid(True, axis='y', alpha=0.2, linestyle='-', linewidth=0.5)
    ax1.set_axisbelow(True)
    ax2.set_axisbelow(True)

    # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
    plt.tight_layout()
    plt.savefig(
        save_image_path,
        dpi=300,
        bbox_inches='tight',
        facecolor='white'
    )
    plt.close()  # å…³é—­ç”»å¸ƒé‡Šæ”¾èµ„æº

    # -------------------------- 5. ç”ŸæˆæŠ¥å‘Šå†…å®¹ --------------------------
    # åŸºç¡€ä¿¡æ¯
    analysis_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    total_data_points = len(df_clean)

    # æ„å»ºæŠ¥å‘Š
    report_lines = []
    report_lines.append('=' * 80)
    report_lines.append('æ–­è†œé¢„æµ‹æŠ¥å‘Š')
    report_lines.append('=' * 80)
    report_lines.append(f'åˆ†ææ—¶é—´: {analysis_time}')
    report_lines.append(f'æ•°æ®ç‚¹æ•°: {total_data_points}')
    report_lines.append('')
    report_lines.append('ã€é¢„æµ‹ç»Ÿè®¡ã€‘ ï¼ˆåªç»Ÿè®¡å¼‚å¸¸æ¦‚ç‡å¤§äº0.5çš„ï¼‰')
    report_lines.append('-' * 80)
    report_lines.append(f'{"æ—¶é—´":<40} {"é¢„æµ‹æ¦‚ç‡":<20} {"TOP5å½±å“å› å­"}')
    #report_lines.append(f'{"(SAVETIMEåˆ—)":<40} {"ï¼ˆå¼‚å¸¸æ¦‚ç‡åˆ—ï¼‰":<20} {"(åç¦»æ¨¡å¼åˆ—)"}')
    report_lines.append('')

    # æ·»åŠ é«˜æ¦‚ç‡æ•°æ®è¡Œ
    if len(high_prob_points) > 0:
        for _, row in high_prob_points.iterrows():
            time_str = row['SAVETIME'].strftime('%Y-%m-%d %H:%M:%S')
            prob_str = f"{row['å¼‚å¸¸æ¦‚ç‡']:.4f}"
            factor_str = str(row['åç¦»æ¨¡å¼']) if pd.notna(row['åç¦»æ¨¡å¼']) else 'æ— '
            report_lines.append(f'{time_str:<40} {prob_str:<20} {factor_str}')
    else:
        report_lines.append('æ— å¼‚å¸¸æ¦‚ç‡å¤§äº0.5çš„æ•°æ®è®°å½•')

    report_lines.append('')
    report_lines.append('=' * 80)
    report_lines.append('æŠ¥å‘Šç»“æŸ')
    report_lines.append('=' * 80)

    # æ‹¼æ¥æŠ¥å‘Šå­—ç¬¦ä¸²
    report_content = '\n'.join(report_lines)

    # -------------------------- 6. è¿”å›ç»“æœ --------------------------
    return save_image_path, report_content



def clean_numeric_column(series):
    """æ¸…ç†æ•°å€¼åˆ—ï¼šå»é™¤éæ•°å­—å­—ç¬¦ï¼Œè½¬æ¢ä¸ºæ•°å€¼å‹ï¼Œå¤„ç†æ— æ³•è½¬æ¢çš„å€¼"""
    series = series.astype(str).replace(
        r'[^\d\.\-]', '', regex=True  # ä¿ç•™æ•°å­—ã€å°æ•°ç‚¹ã€è´Ÿå·
    )
    series = pd.to_numeric(series, errors='coerce')
    return series


def calculate_feature_importance(clf, X, feature_names, n_repeats=3, random_state=42):
    """è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆæ’åˆ—é‡è¦æ€§ï¼‰ï¼Œç”¨äºç¡®å®šåç¦»å› å­æ’å"""
    print("\n=== è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆæ’åˆ—é‡è¦æ€§ï¼‰===")
    # 1. åŸå§‹åŸºå‡†åˆ†æ•°
    original_scores = clf.decision_function(X)
    original_mean_score = np.mean(original_scores)

    # 2. é€ä¸ªç‰¹å¾æ‰“ä¹±è®¡ç®—é‡è¦æ€§
    importance_scores = []
    avg_score_change_list = []  # æ–°å¢ï¼šå•ç‹¬å­˜å‚¨å¾—åˆ†åˆ—è¡¨ç”¨äºæ±‚å’Œ
    for i, feature in enumerate(feature_names):
        if i % 10 == 0:
            print(f"å¤„ç†ç¬¬ {i + 1}/{len(feature_names)} ä¸ªç‰¹å¾...")

        score_changes = []
        for _ in range(n_repeats):
            X_shuffled = X.copy()
            X_shuffled[:, i] = shuffle(X_shuffled[:, i], random_state=random_state + _)
            shuffled_scores = clf.decision_function(X_shuffled)
            score_change = abs(original_mean_score - np.mean(shuffled_scores))
            score_changes.append(score_change)

        avg_score_change = np.mean(score_changes)
        avg_score_change_list.append(avg_score_change)  # æ”¶é›†å¾—åˆ†
        importance_scores.append({
            'ç‰¹å¾å': feature,
            'é‡è¦æ€§å¾—åˆ†': avg_score_change,
            'é‡è¦æ€§å½’ä¸€åŒ–å¾—åˆ†': 0.0  # å…ˆåˆå§‹åŒ–ï¼Œåç»­ç»Ÿä¸€è®¡ç®—
        })

    # 3. ç»Ÿä¸€è®¡ç®—å½’ä¸€åŒ–å¾—åˆ†ï¼ˆä¿®å¤æ ¸å¿ƒé”™è¯¯ï¼‰
    total_score = sum(avg_score_change_list) if sum(avg_score_change_list) > 0 else 1e-8
    for idx, item in enumerate(importance_scores):
        item['é‡è¦æ€§å½’ä¸€åŒ–å¾—åˆ†'] = avg_score_change_list[idx] / total_score

    # 4. æ’åºå¹¶ä¿å­˜
    importance_df = pd.DataFrame(importance_scores)
    importance_df = importance_df.sort_values('é‡è¦æ€§å¾—åˆ†', ascending=False).reset_index(drop=True)
    importance_df['é‡è¦æ€§æ’å'] = range(1, len(importance_df) + 1)

    return importance_df


def get_sample_top5_deviation(sample, feature_importance, scaler, numeric_cols):
    """
    è®¡ç®—å•ä¸ªæ ·æœ¬çš„Top5åç¦»å› å­
    :param sample: å•æ¡æ ·æœ¬æ•°æ®ï¼ˆSeriesï¼‰
    :param feature_importance: ç‰¹å¾é‡è¦æ€§DataFrame
    :param scaler: æ ‡å‡†åŒ–å™¨
    :param numeric_cols: æ•°å€¼ç‰¹å¾åˆ—è¡¨
    :return: Top5åç¦»å› å­åˆ—è¡¨ï¼Œæ ¼å¼ä¸º[['æ’å', 'ç‰¹å¾å', 'å¾—åˆ†'], ...]
    """
    # 1. è®¡ç®—æ¯ä¸ªç‰¹å¾çš„åç¦»åº¦ï¼ˆæ ·æœ¬å€¼ä¸è®­ç»ƒå‡å€¼çš„æ ‡å‡†å·®å€æ•°ï¼‰
    deviation_scores = []
    for feat in numeric_cols:
        if feat in sample.index:
            try:
                # è®­ç»ƒæ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®
                feat_idx = numeric_cols.index(feat)
                train_mean = scaler.mean_[feat_idx]
                train_std = scaler.scale_[feat_idx] if scaler.scale_[feat_idx] != 0 else 1e-8

                # åç¦»åº¦ = (æ ·æœ¬å€¼ - å‡å€¼) / æ ‡å‡†å·®ï¼ˆç»å¯¹å€¼ï¼‰
                feat_value = sample[feat] if not pd.isna(sample[feat]) else 0
                deviation = abs((feat_value - train_mean) / train_std)

                # ç»“åˆç‰¹å¾é‡è¦æ€§çš„åŠ æƒå¾—åˆ† = åç¦»åº¦ * ç‰¹å¾é‡è¦æ€§
                feat_importance_row = feature_importance[feature_importance['ç‰¹å¾å'] == feat]
                if not feat_importance_row.empty:
                    feat_importance = feat_importance_row['é‡è¦æ€§å¾—åˆ†'].values[0]
                    weighted_score = deviation * feat_importance

                    deviation_scores.append({
                        'ç‰¹å¾å': feat,
                        'åç¦»åº¦': deviation,
                        'é‡è¦æ€§å¾—åˆ†': feat_importance,
                        'åŠ æƒå¾—åˆ†': weighted_score,
                        'é‡è¦æ€§æ’å': feat_importance_row['é‡è¦æ€§æ’å'].values[0]
                    })
            except Exception as e:
                # è·³è¿‡è®¡ç®—å‡ºé”™çš„ç‰¹å¾
                continue

    # 2. æŒ‰åŠ æƒå¾—åˆ†æ’åºï¼Œå–Top5
    deviation_df = pd.DataFrame(deviation_scores)
    if not deviation_df.empty:
        deviation_df = deviation_df.sort_values('åŠ æƒå¾—åˆ†', ascending=False).head(5).reset_index(drop=True)
    else:
        return [['æ— ', 'æ— ', '0.0000']]

    # 3. æ ¼å¼åŒ–ç»“æœ
    top5_list = []
    for idx, row in deviation_df.iterrows():
        top5_list.append([
            #str(int(row['é‡è¦æ€§æ’å'])),
            row['ç‰¹å¾å'],
            f"{row['åŠ æƒå¾—åˆ†']:.4f}"
        ])

    # è¡¥å…¨ä¸è¶³5ä¸ªçš„æƒ…å†µ
    while len(top5_list) < 5:
        top5_list.append(['æ— ', 'æ— ', '0.0000'])

    return top5_list


def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# æ–°å¢ï¼šéçº¿æ€§æ˜ å°„å‡½æ•°ï¼Œå¢å¼ºæ¦‚ç‡åŒºåˆ†åº¦
def enhance_probability_discrimination(prob):
    enhanced = np.where(
        prob <= 0.5,
        # å°äºç­‰äº0.5ï¼šå‹ç¼©åˆ°0é™„è¿‘ï¼ˆä¸‰æ¬¡å‡½æ•°ï¼‰
        # (prob / 0.5) ** 6 * 0.5,
        # # å¤§äº0.5ï¼šæ‹‰ä¼¸åˆ°1é™„è¿‘ï¼ˆä¸‰æ¬¡å‡½æ•°ï¼‰
        # 1 - ((1 - prob) / 0.5) ** 6 * 0.5
        sigmoid((prob / 0.5 - 1) * 10),
        # å¤§äº0.5ï¼šæ‹‰ä¼¸åˆ°1é™„è¿‘ï¼ˆSigmoidå³åŠéƒ¨åˆ†ï¼‰
        # æ˜ å°„é€»è¾‘ï¼šprob(0.5â†’1) â†’ x(0â†’6) â†’ sigmoidè¾“å‡º(0.5â†’0.998)
        sigmoid((1 - prob) / 0.5 * 2)
    )
    # å…œåº•é™åˆ¶åœ¨0-1èŒƒå›´å†…
    return np.clip(enhanced, 0, 1)


def predict_new_data(
        new_file_path,SCALER_PATH,PCA_PATH,MODEL_PATH,COLUMNS_PATH,IMPORTANCE_PATH,
        sheet_name=0,
        output_file="new_data_predictions.xlsx",
        time_column="SAVETIME"
):
    """é¢„æµ‹æ–°æ•°æ®ï¼šæ–°å¢å­¤ç«‹æ ·æœ¬çš„Top5åç¦»å› å­å’Œæ¨¡å¼åˆ—"""
    required_files = [MODEL_PATH, SCALER_PATH, COLUMNS_PATH, IMPORTANCE_PATH]
    if os.path.exists(PCA_PATH):
        required_files.append(PCA_PATH)

    if not all(os.path.exists(f) for f in required_files):
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹/ç‰¹å¾é‡è¦æ€§æ–‡ä»¶ï¼Œè¯·å…ˆè®­ç»ƒï¼")
        return None

    try:
        # 1. åŠ è½½æ¨¡å‹å’Œé…ç½®
        scaler = joblib.load(SCALER_PATH)
        clf = joblib.load(MODEL_PATH)
        pca = joblib.load(PCA_PATH) if os.path.exists(PCA_PATH) else None
        feature_importance = pd.read_excel(IMPORTANCE_PATH)

        with open(COLUMNS_PATH, 'r', encoding='utf-8') as f:
            numeric_cols = [line.strip() for line in f.readlines()]

        # 2. è¯»å–æ–°æ•°æ®
        df_new = pd.read_excel(new_file_path, sheet_name=sheet_name)
        time_series = df_new.iloc[:, 0].copy()
        df_data = df_new.iloc[:, 1:].copy()
        time_series = time_series.iloc[1:].reset_index(drop=True)
        df_data = df_data.iloc[1:].reset_index(drop=True)

        # 3. é¢„å¤„ç†æ–°æ•°æ®
        df_data_cleaned = df_data.copy()
        for col in numeric_cols:
            if col in df_data_cleaned.columns:
                df_data_cleaned[col] = clean_numeric_column(df_data_cleaned[col])
            else:
                df_data_cleaned[col] = 0

        X_new = df_data_cleaned[numeric_cols].copy()
        for col in X_new.columns:
            if X_new[col].notna().sum() > 0:
                #X_new[col].fillna(X_new[col].median(), inplace=True)
                X_new[col] = X_new[col].fillna(X_new[col].median())
            else:
                X_new[col].fillna(0, inplace=True)
        X_new = X_new.fillna(0)

        # æ ‡å‡†åŒ–
        X_new_scaled = scaler.transform(X_new)
        X_new_scaled = np.nan_to_num(X_new_scaled, nan=0.0, posinf=0.0, neginf=0.0)

        # PCAé™ç»´
        if pca is not None:
            X_new_pca = pca.transform(X_new_scaled)
        else:
            X_new_pca = X_new_scaled

        # 4. é¢„æµ‹
        anomaly_labels = clf.predict(X_new_pca)
        isolation_score_new = clf.decision_function(X_new_pca)

        # è®¡ç®—å¼‚å¸¸æ¦‚ç‡ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šå¢å¼ºåŒºåˆ†åº¦ï¼‰
        normalized_score_new = (isolation_score_new - (-1)) / (1 - (-1))
        raw_prob_new = 1 - normalized_score_new
        # å¢å¼ºæ¦‚ç‡åŒºåˆ†åº¦ï¼š<0.5é è¿‘0ï¼Œ>0.5é è¿‘1
        anomaly_prob = enhance_probability_discrimination(raw_prob_new)
        anomaly_prob = anomaly_prob.clip(0, 1)

        # 5. æ„å»ºç»“æœ
        df_result = pd.DataFrame({
            time_column: time_series,
            **df_data_cleaned.to_dict('series'),
            'å¼‚å¸¸æ ‡ç­¾': anomaly_labels,
            'å­¤ç«‹ç¨‹åº¦åˆ†æ•°': isolation_score_new,
            'å¼‚å¸¸æ¦‚ç‡': anomaly_prob,
            'æ˜¯å¦å­¤ç«‹': np.where(anomaly_prob > 0.5, "æ˜¯", "å¦")
        })

        # 6. ä¸ºå­¤ç«‹æ ·æœ¬è®¡ç®—Top5åç¦»å› å­å¹¶ç”Ÿæˆæ¨¡å¼åˆ—
        df_result['åç¦»æ¨¡å¼'] = ""
        outlier_mask = df_result['æ˜¯å¦å­¤ç«‹'] == "æ˜¯"

        # éå†æ¯ä¸ªå­¤ç«‹æ ·æœ¬
        top5_summary = []
        for idx, row in df_result[outlier_mask].iterrows():
            # è®¡ç®—è¯¥æ ·æœ¬çš„Top5åç¦»å› å­
            top5_list = get_sample_top5_deviation(
                sample=row,
                feature_importance=feature_importance,
                scaler=scaler,
                numeric_cols=numeric_cols
            )
            # ç”Ÿæˆæ¨¡å¼åˆ—ï¼ˆæ ¼å¼ï¼š{['é‡è¦æ€§æ’å', 'ç‰¹å¾å', 'å¾—åˆ†']}ï¼‰
            df_result.loc[idx, 'åç¦»æ¨¡å¼'] = str(top5_list)
            # æ±‡æ€»Top5ä¿¡æ¯ç”¨äºæ‰“å°
            top5_summary.append({
                'æ—¶é—´': row[time_column],
                'å¼‚å¸¸æ¦‚ç‡': row['å¼‚å¸¸æ¦‚ç‡'],
                'Top5åç¦»å› å­': top5_list
            })

        # 8. æŒ‰æ—¶é—´å‡åºæ’åˆ—
        try:
            df_result[time_column] = pd.to_datetime(df_result[time_column])
            df_result_sorted = df_result.sort_values(by=time_column, ascending=True).reset_index(drop=True)
        except Exception as e:
            df_result_sorted = df_result.sort_values(by=time_column, ascending=True).reset_index(drop=True)

        # 9. æ·»åŠ æ’å
        df_result_sorted['å¼‚å¸¸ç¨‹åº¦æ’å'] = range(1, len(df_result_sorted) + 1)

        # 10. ä¿å­˜ç»“æœ
        df_result_sorted.to_excel(output_file, index=False)


        return df_result_sorted

    except Exception as e:
        print(f"é¢„æµ‹æ–°æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def process_input(selected_model_dir):
    """å¤„ç†å…¨å±€é€‰ä¸­çš„æµ‹è¯•æ–‡ä»¶ï¼Œè¿”å›å›¾è¡¨å’Œç»“æœ"""
    time.sleep(1)
    preset_info = f"æµ‹è¯•æ–‡ä»¶: {selected_preset}" if selected_preset else "æœªé€‰æ‹©æµ‹è¯•æ–‡ä»¶"
    model_info = f"æ¨¡å‹ç›®å½•: {selected_model_dir}"

    SCALER_PATH = os.path.join(selected_model_dir, "scaler.pkl")
    PCA_PATH = os.path.join(selected_model_dir, "pca.pkl")
    MODEL_PATH = os.path.join(selected_model_dir, "isolation_forest_model.pkl")
    COLUMNS_PATH = os.path.join(selected_model_dir, "numeric_columns.txt")
    IMPORTANCE_PATH = os.path.join(selected_model_dir, "feature_importance.xlsx")
    new_file_path = selected_preset
    file_name =f"{BASE_DIR}/output/duanmo_prd/"+str(random.randint(1,2000000))
    output_file= file_name+'.xlsx'
    predict_new_data(
            new_file_path,SCALER_PATH,PCA_PATH,MODEL_PATH,COLUMNS_PATH,IMPORTANCE_PATH,
            sheet_name=0,
            output_file=output_file,
            time_column="SAVETIME",
    )
    # è·å–æŠ¥å‘Šå’Œä½œå›¾å†…å®¹
    save_pic_name,result = generate_membrane_report(output_file,file_name +'.jpg')
    return save_pic_name, f"å¤„ç†å®Œæˆ!\n{result}\n"


def set_selected(file_path, buttons, file_paths):
    """æ›´æ–°é€‰ä¸­çŠ¶æ€ï¼Œä¿®æ”¹æŒ‰é’®æ ·å¼å¹¶æ›´æ–°å…¨å±€å˜é‡"""
    global selected_preset
    selected_preset = file_path

    # è¿”å›æ‰€æœ‰æŒ‰é’®çš„æ ·å¼æ›´æ–°åˆ—è¡¨
    # å¯¹äºæ¯ä¸ªæŒ‰é’®ï¼Œå¦‚æœå®ƒå¯¹åº”çš„æ–‡ä»¶è·¯å¾„ä¸é€‰ä¸­çš„æ–‡ä»¶è·¯å¾„ç›¸åŒï¼Œåˆ™è®¾ç½®ä¸ºprimaryï¼ˆé«˜äº®ï¼‰ï¼Œå¦åˆ™è®¾ç½®ä¸ºsecondaryï¼ˆé»˜è®¤ï¼‰
    return [gr.update(variant="primary" if fp == file_path else "secondary") for fp, btn in zip(file_paths, buttons)]


def create_interface():
    # ä»dataset/cwru_cls_testç›®å½•åŠ¨æ€è¯»å–CSVæ–‡ä»¶
    cwru_dir = os.path.join(os.path.dirname(__file__), "dataset", "duanmo_prd")
    preset_files = {}

    # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–è€…æ­£ç¡®çš„ç›¸å¯¹è·¯å¾„
    if not os.path.exists(cwru_dir):
        # å°è¯•ä½¿ç”¨å…¶ä»–å¯èƒ½çš„è·¯å¾„
        alt_paths = [
            f"{BASE_DIR}/dataset/duanmo_prd",
            "./dataset/duanmo_prd",
            "dataset/duanmo_prd",
        ]
        for path in alt_paths:
            if os.path.exists(path):
                cwru_dir = path
                break

    # è·å–ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶
    if os.path.exists(cwru_dir):
        for file_name in os.listdir(cwru_dir):
            if file_name.endswith('.xlsx'):
                file_path = os.path.join(cwru_dir, file_name)
                preset_files[file_path] = f"ğŸ“„ {file_name}"

    model_dir = os.path.join(os.path.dirname(__file__), "model", "duanmo_prd")
    model_options = []  # å°†ä½¿ç”¨å…ƒç»„åˆ—è¡¨: [(å­ç›®å½•åç§°, å®Œæ•´è·¯å¾„)]

    if not os.path.exists(model_dir):
        # å°è¯•ä½¿ç”¨å…¶ä»–å¯èƒ½çš„è·¯å¾„
        alt_model_paths = [
            f"{BASE_DIR}/model/duanmo_prd",
            "./model/duanmo_prd",
            "model/duanmo_prd",
        ]
        for path in alt_model_paths:
            if os.path.exists(path):
                model_dir = path
                break

    # è·å–ç›®å½•ä¸‹æ‰€æœ‰å­ç›®å½•
    if os.path.exists(model_dir):
        for item in os.listdir(model_dir):
            item_path = os.path.join(model_dir, item)
            if os.path.isdir(item_path):
                # æ·»åŠ å…ƒç»„(æ˜¾ç¤ºæ–‡æœ¬, å®é™…å€¼)
                model_options.append((item, item_path))

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ç›®å½•ï¼Œä½¿ç”¨é»˜è®¤å€¼
    # if not model_options:
    #     default_model_name = "DLinear"
    #     default_model_dir = os.path.join(model_dir, default_model_name)
    #     model_options.append((default_model_name, default_model_dir))

    with gr.Blocks(title="æ–­è†œæ£€æµ‹ä¸è¯Šæ–­ç»¼åˆåº”ç”¨") as demo:
        gr.Markdown("# ğŸš€ æ–­è†œæ£€æµ‹ä¸è¯Šæ–­ç»¼åˆåº”ç”¨")

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### é€‰æ‹©æµ‹è¯•æ–‡ä»¶")

                # åŠ¨æ€åˆ›å»ºæ–‡ä»¶æŒ‰é’®
                buttons = []
                file_paths = list(preset_files.keys())
                for file_path, display_text in preset_files.items():
                    btn = gr.Button(display_text, variant="secondary", size="lg")
                    buttons.append(btn)

                # åœ¨åˆ›å»ºå®Œæ‰€æœ‰æŒ‰é’®åï¼Œä¸ºæ¯ä¸ªæŒ‰é’®ç»‘å®šç‚¹å‡»äº‹ä»¶
                for i, file_path in enumerate(file_paths):
                    buttons[i].click(
                        fn=lambda path=file_path: set_selected(path, buttons, file_paths),
                        inputs=[],
                        outputs=buttons
                    )

                # æ·»åŠ æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
                gr.Markdown("### é€‰æ‹©æ¨¡å‹")
                model_dropdown = gr.Dropdown(
                    choices=model_options,
                    label="æ¨¡å‹åˆ—è¡¨",
                    value=model_options[0][1] if model_options else ""  # ä½¿ç”¨å…ƒç»„çš„ç¬¬äºŒä¸ªå…ƒç´ ä½œä¸ºé»˜è®¤å€¼
                )

                process_btn = gr.Button("å¤„ç†", variant="primary")

            with gr.Column(scale=2):  # æ‰©å¤§ç»“æœå±•ç¤ºåŒºåŸŸ
                gr.Markdown("### é¢„æµ‹æ›²çº¿å›¾")
                plot_output = gr.Image(label="æ•°æ®æ›²çº¿", type="pil")

                gr.Markdown("### å¤„ç†ç»“æœ")
                output_text = gr.Textbox(label="é¢„æµ‹ç»“æœ", lines=6)

        # å¤„ç†æŒ‰é’®äº‹ä»¶ï¼ˆè¿”å›å›¾ç‰‡å’Œæ–‡æœ¬ç»“æœï¼‰
        process_btn.click(
            fn=process_input,
            inputs=[model_dropdown],
            outputs=[plot_output, output_text]
        )

    return demo


def main():
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–ç«¯å£å·ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤ç«¯å£7860
    port = 7861
    if len(sys.argv) > 1:
        try:
            port = int(sys.argv[1])
            if port < 1024 or port > 65535:
                print(f"è­¦å‘Šï¼šç«¯å£å· {port} ä¸åœ¨æœ‰æ•ˆèŒƒå›´å†…(1024-65535)ï¼Œå°†ä½¿ç”¨é»˜è®¤ç«¯å£7860")
                port = 7860
        except ValueError:
            print(f"è­¦å‘Šï¼šæ— æ•ˆçš„ç«¯å£å·å‚æ•° '{sys.argv[1]}'ï¼Œå°†ä½¿ç”¨é»˜è®¤ç«¯å£7860")

    demo = create_interface()
    demo.launch(allowed_paths=[f'{BASE_DIR}/output'],server_name="0.0.0.0", server_port=port, share=False)


if __name__ == "__main__":
    main()