import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import joblib
import warnings
import gradio as gr
import sys
import logging
from pathlib import Path
import io
from PIL import Image
import os
from taosrest import connect
from sklearn.preprocessing import StandardScaler
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.app_utils import AppUtils as util
from utils.app_utils import MultiDirectoryMonitor
warnings.filterwarnings('ignore')
plt = util.auto_config_chinese_font()

# ============================================================
# é…ç½®è·¯å¾„
# ============================================================
BASE_DIR = Path(__file__).parent.parent / "model" / "air_compressor"
MODEL_BASE_DIR = BASE_DIR / "model"
DATA_DIR = BASE_DIR / "data"
EXAMPLE_DIR = BASE_DIR / "examples"
RESTART_SIGNAL_FILENAME = ".restart_signal_air_compressor"

# ============================================================
# æ•°æ®åº“é…ç½®
# ============================================================
DB_CONFIG = {
    "url": "http://192.168.37.160:6041",
    "user": "iot_admin",
    "password": "qihang123.",
    "database": "iot_admin_test"
}

# ============================================================
# æ•…éšœå®šä¹‰
# ============================================================
fault_code_map = {
    1: 'ä¸»æœºè¿‡è½½',
    2: 'ä¸»æœºä¸å¹³è¡¡',
    3: 'é£æœºè¿‡è½½',
    4: 'æ’æ°”æ¸©åº¦é«˜',
    5: 'ä¾›æ°”å‹åŠ›é«˜',
    6: 'ç”µå‹è¿‡ä½',
    7: 'ç”µå‹è¿‡é«˜',
    8: 'ç”µæœºè¿‡è½½',
    9: 'é£æœºè¿‡è½½',
}

# æ•…éšœç‚¹æè¿°å’Œå»ºè®®
fault_point_analysis = {
    # ----------------------- 1: ä¸»æœºè¿‡è½½ (Diagnosis) -----------------------
    1: {
        'name': 'ä¸»æœºè¿‡è½½',
        'fault_points': [
            'ä¸»ç”µæœºè¾“å‡ºç”µæµå¼‚å¸¸å‡é«˜',
            'ä¸»ç”µæœºè¾“å‡ºåŠŸç‡è¶…è´Ÿè·è¿è¡Œ',
            'ä¸»ç”µæœºæ€»åŠŸè€—å·®å€¼å¢å¤§',
            'æ’æ°”æ¸©åº¦ä¸Šå‡',
            'ä¸»ç”µæœºè½¬é€Ÿç•¥æœ‰ä¸‹é™'
        ],
        'root_causes': [
            'è´Ÿè½½è¶…è¿‡è®¾å¤‡é¢å®šåŠŸç‡',
            'å‹ç¼©æœºå†…éƒ¨ç£¨æŸ',
            'æ¶¦æ»‘ç³»ç»Ÿå¼‚å¸¸',
            'è¿›æ°”è¿‡æ»¤å™¨å µå¡'
        ],
        'suggestions': [
            'æ£€æŸ¥å½“å‰è´Ÿè½½æ˜¯å¦è¶…è¿‡è®¾å¤‡é¢å®šå€¼',
            'æ£€æŸ¥å‹ç¼©æœºå†…éƒ¨éƒ¨ä»¶ç£¨æŸæƒ…å†µ',
            'æ£€æŸ¥æ¶¦æ»‘æ²¹ä½å’Œæ²¹è´¨',
            'æ¸…æ´æˆ–æ›´æ¢è¿›æ°”è¿‡æ»¤å™¨'
        ]
    },
    # ----------------------- 101: ä¸»æœºè¿‡è½½ (Warning) -----------------------
    101: {
        'name': 'ä¸»æœºè¿‡è½½ - é¢„è­¦',
        'fault_points': [
            'ä¸»ç”µæœºè¾“å‡ºç”µæµæŒç»­å‡é«˜',
            'ä¸»ç”µæœºè¾“å‡ºåŠŸç‡å¼€å§‹å¢åŠ ',
            'ä¸»ç”µæœºæ€»åŠŸè€—å·®å€¼å¼€å§‹å¢å¤§',
            'æ’æ°”æ¸©åº¦ç•¥å¾®ä¸Šå‡',
            'ä¸»ç”µæœºè½¬é€Ÿè½»å¾®æ³¢åŠ¨'
        ],
        'root_causes': [
            'è´Ÿè½½è¶…è¿‡è®¾å¤‡é¢å®šåŠŸç‡',
            'å‹ç¼©æœºå†…éƒ¨ç£¨æŸ',
            'æ¶¦æ»‘ç³»ç»Ÿå¼‚å¸¸',
            'è¿›æ°”è¿‡æ»¤å™¨å µå¡'
        ],
        'suggestions': [
            'æ£€æŸ¥å½“å‰è´Ÿè½½æ˜¯å¦è¶…è¿‡è®¾å¤‡é¢å®šå€¼',
            'æ£€æŸ¥å‹ç¼©æœºå†…éƒ¨éƒ¨ä»¶ç£¨æŸæƒ…å†µ',
            'æ£€æŸ¥æ¶¦æ»‘æ²¹ä½å’Œæ²¹è´¨',
            'æ¸…æ´æˆ–æ›´æ¢è¿›æ°”è¿‡æ»¤å™¨'
        ]
    },
    # ----------------------- 2: ä¸»æœºä¸å¹³è¡¡ (Diagnosis) -----------------------
    2: {
        'name': 'ä¸»æœºä¸å¹³è¡¡',
        'fault_points': [
            'ä¸‰ç›¸ç”µæµå¹³è¡¡åº¦å¼‚å¸¸',
            'ä¸»ç”µæœºè¾“å‡ºç”µæµæ³¢åŠ¨',
            'ä¸»ç”µæœºè¾“å‡ºç”µå‹ä¸ç¨³å®š',
            'åŠŸè€—å·®å€¼å¼‚å¸¸',
            'æ’æ°”æ¸©åº¦è½»å¾®å‡é«˜'
        ],
        'root_causes': [
            'ä¸‰ç›¸ç”µæºä¸å¹³è¡¡',
            'ç”µæœºç»•ç»„æ•…éšœ',
            'ç”µæœºè½´æ‰¿ç£¨æŸ',
            'è´Ÿè½½åˆ†å¸ƒä¸å‡'
        ],
        'suggestions': [
            'æ£€æŸ¥ä¸‰ç›¸ç”µæºç”µå‹å¹³è¡¡åº¦',
            'æ£€æŸ¥ç”µæœºç»•ç»„ç»ç¼˜å’Œé˜»å€¼',
            'æ£€æŸ¥è½´æ‰¿çŠ¶æ€å’Œæ¶¦æ»‘',
            'è°ƒæ•´è´Ÿè½½åˆ†å¸ƒ'
        ]
    },
    # ----------------------- 102: ä¸»æœºä¸å¹³è¡¡ (Warning) -----------------------
    102: {
        'name': 'ä¸»æœºä¸å¹³è¡¡ - é¢„è­¦',
        'fault_points': [
            'ä¸‰ç›¸ç”µæµå¹³è¡¡åº¦è½»å¾®å¼‚å¸¸',
            'ä¸»ç”µæœºè¾“å‡ºç”µæµå¼€å§‹æ³¢åŠ¨',
            'ä¸»ç”µæœºè¾“å‡ºç”µå‹è½»å¾®æ³¢åŠ¨',
            'åŠŸè€—å·®å€¼å¼€å§‹å˜åŒ–',
            'æ’æ°”æ¸©åº¦ä¿æŒåŸºå‡†æˆ–è½»å¾®æ³¢åŠ¨'
        ],
        'root_causes': [
            'ä¸‰ç›¸ç”µæºä¸å¹³è¡¡',
            'ç”µæœºç»•ç»„æ•…éšœ',
            'ç”µæœºè½´æ‰¿ç£¨æŸ',
            'è´Ÿè½½åˆ†å¸ƒä¸å‡'
        ],
        'suggestions': [
            'æ£€æŸ¥ä¸‰ç›¸ç”µæºç”µå‹å¹³è¡¡åº¦',
            'æ£€æŸ¥ç”µæœºç»•ç»„ç»ç¼˜å’Œé˜»å€¼',
            'æ£€æŸ¥è½´æ‰¿çŠ¶æ€å’Œæ¶¦æ»‘',
            'è°ƒæ•´è´Ÿè½½åˆ†å¸ƒ'
        ]
    },
    # ----------------------- 3: é£æœºè¿‡è½½ (Diagnosis) -----------------------
    3: {
        'name': 'é£æœºè¿‡è½½',
        'fault_points': [
            'é£æœºç”µæœºè¾“å‡ºç”µæµæ˜¾è‘—å‡é«˜',
            'é£æœºç”µæœºè¾“å‡ºåŠŸç‡è¶…æ ‡',
            'é£æœºæ€»åŠŸè€—å·®å€¼å¢å¤§',
            'é£æœºè½¬é€Ÿé™ä½',
            'æ’æ°”æ¸©åº¦æ˜æ˜¾å‡é«˜'
        ],
        'root_causes': [
            'æ•£çƒ­é£æ‰‡å¶ç‰‡æŸåæˆ–ç§¯ç°',
            'é£æœºè½´æ‰¿ç£¨æŸ',
            'é£é“å µå¡',
            'ç¯å¢ƒæ¸©åº¦è¿‡é«˜'
        ],
        'suggestions': [
            'æ¸…æ´é£æ‰‡å¶ç‰‡å’Œæ•£çƒ­å™¨',
            'æ£€æŸ¥é£æœºè½´æ‰¿çŠ¶æ€',
            'æ¸…ç†é£é“éšœç¢ç‰©',
            'æ”¹å–„ç¯å¢ƒé€šé£æ¡ä»¶'
        ]
    },
    # ----------------------- 103: é£æœºè¿‡è½½ (Warning) -----------------------
    103: {
        'name': 'é£æœºè¿‡è½½ - é¢„è­¦',
        'fault_points': [
            'é£æœºç”µæœºè¾“å‡ºç”µæµå‡é«˜',
            'é£æœºç”µæœºè¾“å‡ºåŠŸç‡å¢åŠ ',
            'é£æœºæ€»åŠŸè€—å·®å€¼å¼€å§‹å¢å¤§',
            'é£æœºè½¬é€Ÿç•¥æœ‰é™ä½',
            'æ’æ°”æ¸©åº¦å¼€å§‹ä¸Šå‡'
        ],
        'root_causes': [
            'æ•£çƒ­é£æ‰‡å¶ç‰‡æŸåæˆ–ç§¯ç°',
            'é£æœºè½´æ‰¿ç£¨æŸ',
            'é£é“å µå¡',
            'ç¯å¢ƒæ¸©åº¦è¿‡é«˜'
        ],
        'suggestions': [
            'æ¸…æ´é£æ‰‡å¶ç‰‡å’Œæ•£çƒ­å™¨',
            'æ£€æŸ¥é£æœºè½´æ‰¿çŠ¶æ€',
            'æ¸…ç†é£é“éšœç¢ç‰©',
            'æ”¹å–„ç¯å¢ƒé€šé£æ¡ä»¶'
        ]
    },
    # ----------------------- 4: æ’æ°”æ¸©åº¦é«˜ (Diagnosis) -----------------------
    4: {
        'name': 'æ’æ°”æ¸©åº¦é«˜',
        'fault_points': [
            'æ’æ°”æ¸©åº¦æŒç»­å‡é«˜',
            'ä¸»ç”µæœºè¾“å‡ºç”µæµæ³¢åŠ¨',
            'ä¸»ç”µæœºåŠŸç‡å¢åŠ ',
            'é£æœºè½¬é€Ÿæ˜¾è‘—å‡é«˜',
            'ä¾›æ°”å‹åŠ›å˜åŒ–'
        ],
        'root_causes': [
            'å†·å´ç³»ç»Ÿæ•ˆç‡ä¸‹é™',
            'ç¯å¢ƒæ¸©åº¦è¿‡é«˜',
            'å‹ç¼©æ¯”è¿‡å¤§',
            'æ¶¦æ»‘æ²¹å†·å´æ•ˆæœå·®'
        ],
        'suggestions': [
            'æ£€æŸ¥å†·å´å™¨æ¸…æ´åº¦å’Œæ•ˆç‡',
            'æ”¹å–„ç¯å¢ƒé€šé£å’Œæ¸©åº¦',
            'è°ƒæ•´å‹ç¼©æœºå·¥ä½œå‹åŠ›',
            'æ£€æŸ¥æ¶¦æ»‘æ²¹æ¸©åº¦å’Œæµé‡'
        ]
    },
    # ----------------------- 104: æ’æ°”æ¸©åº¦é«˜ (Warning) -----------------------
    104: {
        'name': 'æ’æ°”æ¸©åº¦é«˜ - é¢„è­¦',
        'fault_points': [
            'æ’æ°”æ¸©åº¦å¼€å§‹ä¸Šå‡',
            'ä¸»ç”µæœºè¾“å‡ºç”µæµè½»å¾®æ³¢åŠ¨',
            'ä¸»ç”µæœºåŠŸç‡è½»å¾®å¢åŠ ',
            'é£æœºè½¬é€Ÿå‡é«˜',
            'ä¾›æ°”å‹åŠ›è½»å¾®æ³¢åŠ¨'
        ],
        'root_causes': [
            'å†·å´ç³»ç»Ÿæ•ˆç‡ä¸‹é™',
            'ç¯å¢ƒæ¸©åº¦è¿‡é«˜',
            'å‹ç¼©æ¯”è¿‡å¤§',
            'æ¶¦æ»‘æ²¹å†·å´æ•ˆæœå·®'
        ],
        'suggestions': [
            'æ£€æŸ¥å†·å´å™¨æ¸…æ´åº¦å’Œæ•ˆç‡',
            'æ”¹å–„ç¯å¢ƒé€šé£å’Œæ¸©åº¦',
            'è°ƒæ•´å‹ç¼©æœºå·¥ä½œå‹åŠ›',
            'æ£€æŸ¥æ¶¦æ»‘æ²¹æ¸©åº¦å’Œæµé‡'
        ]
    },
    # ----------------------- 5: ä¾›æ°”å‹åŠ›é«˜ (Diagnosis) -----------------------
    5: {
        'name': 'ä¾›æ°”å‹åŠ›é«˜',
        'fault_points': [
            'ä¾›æ°”å‹åŠ›è¶…å‡ºæ­£å¸¸èŒƒå›´',
            'ä¸»ç”µæœºè¾“å‡ºåŠŸç‡å¢åŠ ',
            'ä¸»ç”µæœºè¾“å‡ºç”µæµå‡é«˜',
            'æ’æ°”æ¸©åº¦å‡é«˜'
        ],
        'root_causes': [
            'ç”¨æ°”é‡å‡å°‘å¯¼è‡´å‹åŠ›ä¸Šå‡',
            'å‹åŠ›æ§åˆ¶å™¨è®¾å®šå€¼è¿‡é«˜',
            'å¸è½½é˜€æ•…éšœ',
            'ç®¡è·¯é˜»åŠ›å¢å¤§'
        ],
        'suggestions': [
            'æ£€æŸ¥ç”¨æ°”é‡å’Œå‹åŠ›æ§åˆ¶å™¨è®¾å®š',
            'æ ¡å‡†å‹åŠ›ä¼ æ„Ÿå™¨',
            'æ£€æŸ¥å¸è½½é˜€åŠ¨ä½œæ˜¯å¦æ­£å¸¸',
            'æ£€æŸ¥ç®¡è·¯æ˜¯å¦æœ‰å µå¡'
        ]
    },
    # ----------------------- 105: ä¾›æ°”å‹åŠ›é«˜ (Warning) -----------------------
    105: {
        'name': 'ä¾›æ°”å‹åŠ›é«˜ - é¢„è­¦',
        'fault_points': [
            'ä¾›æ°”å‹åŠ›ç•¥å¾®ä¸Šå‡',
            'ä¸»ç”µæœºè¾“å‡ºåŠŸç‡è½»å¾®å¢åŠ ',
            'ä¸»ç”µæœºè¾“å‡ºç”µæµè½»å¾®å‡é«˜',
            'æ’æ°”æ¸©åº¦è½»å¾®ä¸Šå‡'
        ],
        'root_causes': [
            'ç”¨æ°”é‡å‡å°‘å¯¼è‡´å‹åŠ›ä¸Šå‡',
            'å‹åŠ›æ§åˆ¶å™¨è®¾å®šå€¼è¿‡é«˜',
            'å¸è½½é˜€æ•…éšœ',
            'ç®¡è·¯é˜»åŠ›å¢å¤§'
        ],
        'suggestions': [
            'æ£€æŸ¥ç”¨æ°”é‡å’Œå‹åŠ›æ§åˆ¶å™¨è®¾å®š',
            'æ ¡å‡†å‹åŠ›ä¼ æ„Ÿå™¨',
            'æ£€æŸ¥å¸è½½é˜€åŠ¨ä½œæ˜¯å¦æ­£å¸¸',
            'æ£€æŸ¥ç®¡è·¯æ˜¯å¦æœ‰å µå¡'
        ]
    },
    # ----------------------- 6: ç”µå‹è¿‡ä½ (Diagnosis) -----------------------
    6: {
        'name': 'ç”µå‹è¿‡ä½',
        'fault_points': [
            'è¾“å…¥ç”µå‹ä½äºæ­£å¸¸èŒƒå›´',
            'ä¸»ç”µæœºè¾“å‡ºç”µå‹é™ä½',
            'é£æœºè¾“å‡ºç”µå‹é™ä½',
            'ä¸»ç”µæœºç”µæµå‡é«˜',
            'ä¸»ç”µæœºè½¬é€Ÿä¸‹é™'
        ],
        'root_causes': [
            'ç”µç½‘ç”µå‹æ³¢åŠ¨',
            'ä¾›ç”µçº¿è·¯å‹é™è¿‡å¤§',
            'å˜å‹å™¨å®¹é‡ä¸è¶³',
            'ç”µç¼†æˆªé¢ç§¯è¿‡å°'
        ],
        'suggestions': [
            'æ£€æŸ¥ç”µç½‘ä¾›ç”µè´¨é‡',
            'æ£€æŸ¥ä¾›ç”µçº¿è·¯å’Œæ¥å¤´',
            'è¯„ä¼°å˜å‹å™¨å®¹é‡æ˜¯å¦è¶³å¤Ÿ',
            'æ£€æŸ¥ç”µç¼†è§„æ ¼æ˜¯å¦åŒ¹é…'
        ]
    },
    # ----------------------- 106: ç”µå‹è¿‡ä½ (Warning) -----------------------
    106: {
        'name': 'ç”µå‹è¿‡ä½ - é¢„è­¦',
        'fault_points': [
            'è¾“å…¥ç”µå‹ç•¥å¾®ä¸‹é™',
            'ä¸»ç”µæœºè¾“å‡ºç”µå‹è½»å¾®é™ä½',
            'é£æœºè¾“å‡ºç”µå‹è½»å¾®é™ä½',
            'ä¸»ç”µæœºç”µæµç•¥å¾®å‡é«˜',
            'ä¸»ç”µæœºè½¬é€Ÿè½»å¾®ä¸‹é™'
        ],
        'root_causes': [
            'ç”µç½‘ç”µå‹æ³¢åŠ¨',
            'ä¾›ç”µçº¿è·¯å‹é™è¿‡å¤§',
            'å˜å‹å™¨å®¹é‡ä¸è¶³',
            'ç”µç¼†æˆªé¢ç§¯è¿‡å°'
        ],
        'suggestions': [
            'æ£€æŸ¥ç”µç½‘ä¾›ç”µè´¨é‡',
            'æ£€æŸ¥ä¾›ç”µçº¿è·¯å’Œæ¥å¤´',
            'è¯„ä¼°å˜å‹å™¨å®¹é‡æ˜¯å¦è¶³å¤Ÿ',
            'æ£€æŸ¥ç”µç¼†è§„æ ¼æ˜¯å¦åŒ¹é…'
        ]
    },
    # ----------------------- 7: ç”µå‹è¿‡é«˜ (Diagnosis) -----------------------
    7: {
        'name': 'ç”µå‹è¿‡é«˜',
        'fault_points': [
            'è¾“å…¥ç”µå‹é«˜äºæ­£å¸¸èŒƒå›´',
            'ä¸»ç”µæœºè¾“å‡ºç”µå‹å‡é«˜',
            'é£æœºè¾“å‡ºç”µå‹å‡é«˜',
            'ä¸»ç”µæœºç”µæµé™ä½',
            'ä¸»ç”µæœºè½¬é€Ÿç•¥å¾®å‡é«˜'
        ],
        'root_causes': [
            'ç”µç½‘ç”µå‹è°ƒèŠ‚ä¸å½“',
            'å˜å‹å™¨åˆ†æ¥å¼€å…³ä½ç½®ä¸å¯¹',
            'æ— åŠŸè¡¥å¿è¿‡åº¦',
            'è½»è½½æ—¶ç”µå‹ä¸Šå‡'
        ],
        'suggestions': [
            'è”ç³»ä¾›ç”µéƒ¨é—¨è°ƒæ•´ç”µå‹',
            'è°ƒæ•´å˜å‹å™¨åˆ†æ¥å¼€å…³',
            'æ£€æŸ¥æ— åŠŸè¡¥å¿è£…ç½®',
            'å®‰è£…ç¨³å‹è£…ç½®'
        ]
    },
    # ----------------------- 107: ç”µå‹è¿‡é«˜ (Warning) -----------------------
    107: {
        'name': 'ç”µå‹è¿‡é«˜ - é¢„è­¦',
        'fault_points': [
            'è¾“å…¥ç”µå‹ç•¥å¾®å‡é«˜',
            'ä¸»ç”µæœºè¾“å‡ºç”µå‹è½»å¾®å‡é«˜',
            'é£æœºè¾“å‡ºç”µå‹è½»å¾®å‡é«˜',
            'ä¸»ç”µæœºç”µæµç•¥å¾®é™ä½',
            'ä¸»ç”µæœºè½¬é€Ÿè½»å¾®æ³¢åŠ¨'
        ],
        'root_causes': [
            'ç”µç½‘ç”µå‹è°ƒèŠ‚ä¸å½“',
            'å˜å‹å™¨åˆ†æ¥å¼€å…³ä½ç½®ä¸å¯¹',
            'æ— åŠŸè¡¥å¿è¿‡åº¦',
            'è½»è½½æ—¶ç”µå‹ä¸Šå‡'
        ],
        'suggestions': [
            'è”ç³»ä¾›ç”µéƒ¨é—¨è°ƒæ•´ç”µå‹',
            'è°ƒæ•´å˜å‹å™¨åˆ†æ¥å¼€å…³',
            'æ£€æŸ¥æ— åŠŸè¡¥å¿è£…ç½®',
            'å®‰è£…ç¨³å‹è£…ç½®'
        ]
    }
}

fault_definitions = {
    1: {
        'name': 'ä¸»æœºè¿‡è½½',
        'duration_range': (30, 120),
        'affected_params': {
            'main_motor_output_current': {'baseline_shift': 1.8, 'volatility': 0.15, 'trend': 0.02},
            'main_motor_output_power': {'baseline_shift': 1.75, 'volatility': 0.12, 'trend': 0.015},
            'main_motor_total_power_consumption_diff': {'baseline_shift': 1.9, 'volatility': 0.2, 'trend': 0.03},
            'exhaust_temperature': {'baseline_shift': 1.25, 'volatility': 0.1, 'trend': 0.01},
            'main_motor_speed': {'baseline_shift': 0.98, 'volatility': 0.05, 'trend': -0.005}
        }
    },
    2: {
        'name': 'ä¸»æœºä¸å¹³è¡¡',
        'duration_range': (20, 90),
        'affected_params': {
            'current_balance_degree': {'baseline_shift': 1.7, 'volatility': 0.3, 'trend': -0.05},
            'main_motor_output_current': {'baseline_shift': 0.7, 'volatility': 0.25, 'trend': 0.0, 'oscillate': 0.2},
            'main_motor_output_voltage': {'baseline_shift': 0.9, 'volatility': 0.2, 'trend': 0.0, 'oscillate': 0.1},
            'main_motor_total_power_consumption_diff': {'baseline_shift': 0.85, 'volatility': 0.2, 'trend': 0.01},
            'exhaust_temperature': {'baseline_shift': 0.8, 'volatility': 0.15, 'trend': 0.005},
        }
    },
    3: {
        'name': 'é£æœºè¿‡è½½',
        'duration_range': (25, 100),
        'affected_params': {
            'fan_motor_output_current': {'baseline_shift': 1.7, 'volatility': 0.2, 'trend': 0.015},
            'fan_motor_output_power': {'baseline_shift': 1.6, 'volatility': 0.18, 'trend': 0.015},
            'fan_motor_total_power_consumption_diff': {'baseline_shift': 1.8, 'volatility': 0.2, 'trend': 0.02},
            'fan_motor_speed': {'baseline_shift': 0.6, 'volatility': 0.08, 'trend': -0.01},
            'exhaust_temperature': {'baseline_shift': 1.35, 'volatility': 0.15, 'trend': 0.012},
        }
    },
    4: {
        'name': 'æ’æ°”æ¸©åº¦é«˜',
        'duration_range': (40, 150),
        'affected_params': {
            'main_motor_output_current': {'baseline_shift': 1.2, 'volatility': 0.25, 'trend': 0.0, 'oscillate': 0.2},
            'main_motor_output_power': {'baseline_shift': 1.2, 'volatility': 0.08, 'trend': 0.001},
            'supply_pressure': {'baseline_shift': 1.0, 'volatility': 0.2, 'trend': 0},
            'fan_motor_speed': {'baseline_shift': 1.6, 'volatility': 0.1, 'trend': 0.002},
            'exhaust_temperature': {'baseline_shift': 1.4, 'volatility': 0.12, 'trend': 0.005},
            'main_motor_total_power_consumption_diff': {'baseline_shift': 1.54, 'volatility': 0.2, 'trend': 0.03},
        }
    },
    5: {
        'name': 'ä¾›æ°”å‹åŠ›é«˜',
        'duration_range': (30, 120),
        'affected_params': {
            'supply_pressure': {'baseline_shift': 1.3, 'volatility': 0.1, 'trend': 0.002},
            'main_motor_output_power': {'baseline_shift': 1.25, 'volatility': 0.1, 'trend': 0.0015},
            'main_motor_output_current': {'baseline_shift': 1.2, 'volatility': 0.1, 'trend': 0.0015},
            'exhaust_temperature': {'baseline_shift': 1.15, 'volatility': 0.08, 'trend': 0.001}
        }
    },
    6: {
        'name': 'ç”µå‹è¿‡ä½',
        'duration_range': (15, 80),
        'affected_params': {
            'voltage': {'baseline_shift': 0.85, 'volatility': 0.06, 'trend': -0.0005},
            'main_motor_output_voltage': {'baseline_shift': 0.87, 'volatility': 0.06, 'trend': -0.0005},
            'fan_motor_output_voltage': {'baseline_shift': 0.88, 'volatility': 0.06, 'trend': -0.0005},
            'main_motor_output_current': {'baseline_shift': 1.2, 'volatility': 0.1, 'trend': 0.001},
            'fan_motor_output_current': {'baseline_shift': 1.18, 'volatility': 0.1, 'trend': 0.001},
            'main_motor_speed': {'baseline_shift': 0.97, 'volatility': 0.05, 'trend': -0.001},
            'main_motor_output_power': {'baseline_shift': 0.95, 'volatility': 0.1, 'trend': -0.001},
        }
    },
    7: {
        'name': 'ç”µå‹è¿‡é«˜',
        'duration_range': (15, 80),
        'affected_params': {
            'voltage': {'baseline_shift': 1.12, 'volatility': 0.06, 'trend': 0.0005},
            'main_motor_output_voltage': {'baseline_shift': 1.1, 'volatility': 0.06, 'trend': 0.0005},
            'fan_motor_output_voltage': {'baseline_shift': 1.1, 'volatility': 0.06, 'trend': 0.0005},
            'main_motor_output_current': {'baseline_shift': 0.9, 'volatility': 0.08, 'trend': -0.0005},
            'fan_motor_output_current': {'baseline_shift': 0.92, 'volatility': 0.08, 'trend': -0.0005},
            'main_motor_speed': {'baseline_shift': 1.02, 'volatility': 0.04, 'trend': 0.0001},
            'exhaust_temperature': {'baseline_shift': 1.03, 'volatility': 0.1, 'trend': 0.0002}
        }
    }
}

def fetch_normal_data_from_db(start_time, end_time):
    """ä»æ•°æ®åº“è·å–æ­£å¸¸æ•°æ®"""
    try:
        logging.info(f"å¼€å§‹è¿æ¥æ•°æ®åº“...")
        conn = connect(
            url=DB_CONFIG["url"],
            user=DB_CONFIG["user"],
            password=DB_CONFIG["password"],
            database=DB_CONFIG["database"]
        )
        logging.info("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸï¼")
        
        query_sql = f"""
        SELECT
        `_ts` as `timestamp`,
        `d01` as `supply_pressure`,
        `d02` as `exhaust_temperature`,
        `d05` as `main_motor_current_a`,
        `d06` as `main_motor_current_b`,
        `d07` as `main_motor_current_c`,
        `d25` as `voltage`,
        `d33` as `main_motor_output_voltage`,
        `d34` as `main_motor_output_current`,
        `d35` as `main_motor_output_frequency`,
        `d36` as `main_motor_output_power`,
        `d37` as `fan_motor_output_voltage`,
        `d38` as `fan_motor_output_current`,
        `d39` as `fan_motor_output_frequency`,
        `d40` as `fan_motor_output_power`,
        `d41` as `main_motor_speed`,
        `d42` as `fan_motor_speed`,
        `d78` as `main_motor_total_power_consumption_h`,
        `d79` as `main_motor_total_power_consumption_l`,
        `d80` as `fan_motor_total_power_consumption_h`,
        `d81` as `fan_motor_total_power_consumption_l`
        FROM
        `iot_admin_test`.`iot_device_product_property_kongyaji_yxcs` 
        WHERE `_ts` >= '{start_time}' AND `_ts` < '{end_time}'
        """
        
        logging.info(f"æ‰§è¡ŒæŸ¥è¯¢: {start_time} åˆ° {end_time}")
        result = conn.query(query_sql)
        
        if result.field_count > 0:
            columns = [field['name'] for field in result.fields]
            raw_data = []
            for row in result:
                raw_data.append(row)
            
            df_raw = pd.DataFrame(raw_data, columns=columns)
            logging.info(f"âœ“ æŸ¥è¯¢æˆåŠŸï¼ŒåŸå§‹æ•°æ®: {df_raw.shape[0]} è¡Œ")
            
            df_raw['main_motor_total_power_consumption'] = (
                df_raw['main_motor_total_power_consumption_h'] * 65536 / 100 + 
                df_raw['main_motor_total_power_consumption_l'] / 100
            )
            df_raw['fan_motor_total_power_consumption'] = (
                df_raw['fan_motor_total_power_consumption_h'] * 65536 / 100 + 
                df_raw['fan_motor_total_power_consumption_l'] / 100
            )
            
            df_raw = df_raw.drop(columns=[
                'main_motor_total_power_consumption_h',
                'main_motor_total_power_consumption_l',
                'fan_motor_total_power_consumption_h',
                'fan_motor_total_power_consumption_l'
            ])
            
            current_cols = ['main_motor_current_a', 'main_motor_current_b', 'main_motor_current_c']
            df_raw['current_max'] = df_raw[current_cols].max(axis=1)
            df_raw['current_min'] = df_raw[current_cols].min(axis=1)
            df_raw['current_balance_degree'] = (
                (df_raw['current_max'] / df_raw['current_min']) / (1 + (14/10))
            )
            
            df_raw = df_raw.drop(columns=[
                'current_max', 'current_min',
                'main_motor_current_a', 'main_motor_current_b', 'main_motor_current_c'
            ])
            
            df_clean = df_raw.dropna()
            
            df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])
            df_clean = df_clean.set_index('timestamp')
            df_clean = df_clean.sort_index()
            df_clean = df_clean.resample('5min').mean()
            
            df_clean['main_motor_total_power_consumption_diff'] = df_clean['main_motor_total_power_consumption'].diff()
            df_clean['fan_motor_total_power_consumption_diff'] = df_clean['fan_motor_total_power_consumption'].diff()
            
            df_clean = df_clean.drop(columns=[
                'main_motor_total_power_consumption',
                'fan_motor_total_power_consumption'
            ])
            
            df_clean = df_clean.dropna()
            df_clean = df_clean.reset_index()
            
            conn.close()
            logging.info(f"âœ“ æ•°æ®å¤„ç†å®Œæˆï¼Œæœ€ç»ˆæ•°æ®: {df_clean.shape[0]} è¡Œ")
            
            return df_clean, None
            
        else:
            conn.close()
            return None, "æŸ¥è¯¢è¿”å›ç©ºç»“æœ"
            
    except Exception as e:
        error_msg = f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"
        logging.error(error_msg, exc_info=True)
        return None, error_msg

def generate_fault_data(normal_df, fault_code, fault_definitions, 
                       num_pre_warning_samples=None, num_fault_samples=None,
                       pre_warning_severity=0.5, generate_pre_warning=True):
    """ç”Ÿæˆæ•…éšœæ•°æ®"""
    if fault_code not in fault_definitions:
        raise ValueError(f"æœªçŸ¥çš„æ•…éšœä»£ç : {fault_code}")
    
    seed = int(datetime.now().timestamp())
    fault_info = fault_definitions[fault_code]
    
    total_samples = len(normal_df)
    if num_pre_warning_samples is None:
        num_pre_warning_samples = int(total_samples * 0.3)
    if num_fault_samples is None:
        num_fault_samples = total_samples - num_pre_warning_samples
    
    all_data_dfs = []
    
    if generate_pre_warning and num_pre_warning_samples > 0:
        df_pre_warning = normal_df.sample(n=num_pre_warning_samples, random_state=seed + fault_code).copy()
        
        for param, effects in fault_info['affected_params'].items():
            if param not in df_pre_warning.columns:
                continue
            
            s = pre_warning_severity
            pre_warning_effects = {
                'baseline_shift': 1 + (effects.get('baseline_shift', 1) - 1) * s,
                'volatility': effects.get('volatility', 0) * s,
                'trend': effects.get('trend', 0) * s,
                'oscillate': effects.get('oscillate', 0) * s,
            }
            
            original_series = df_pre_warning[param].copy()
            modified_series = original_series.copy()
            
            modified_series *= pre_warning_effects['baseline_shift']
            
            if pre_warning_effects['volatility'] > 0:
                noise = np.random.normal(0, original_series.std() * pre_warning_effects['volatility'], 
                                        size=len(modified_series))
                modified_series += noise
            
            if pre_warning_effects['trend'] != 0:
                trend_effect = np.linspace(0, pre_warning_effects['trend'], 
                                          len(modified_series)) * original_series.mean()
                modified_series += trend_effect
            
            if pre_warning_effects['oscillate'] > 0:
                oscillation = np.sin(np.linspace(0, 10*np.pi, len(modified_series))) * \
                             original_series.mean() * pre_warning_effects['oscillate']
                modified_series += oscillation
            
            df_pre_warning[param] = modified_series
        
        df_pre_warning['fault_code'] = 100 + fault_code
        df_pre_warning['fault_name'] = f"{fault_info['name']}-é¢„è­¦"
        all_data_dfs.append(df_pre_warning)
    
    if num_fault_samples > 0:
        df_fault = normal_df.sample(n=num_fault_samples, random_state=seed + fault_code + 1000).copy()
        
        for param, effects in fault_info['affected_params'].items():
            if param not in df_fault.columns:
                continue
            
            original_series = df_fault[param].copy()
            modified_series = original_series.copy()
            
            modified_series *= effects.get('baseline_shift', 1)
            
            if 'volatility' in effects:
                noise = np.random.normal(0, original_series.std() * effects['volatility'], 
                                        size=len(modified_series))
                modified_series += noise
            
            if 'trend' in effects:
                trend_effect = np.linspace(0, effects['trend'], 
                                          len(modified_series)) * original_series.mean()
                modified_series += trend_effect
            
            if 'oscillate' in effects:
                oscillation = np.sin(np.linspace(0, 10*np.pi, len(modified_series))) * \
                             original_series.mean() * effects['oscillate']
                modified_series += oscillation
            
            df_fault[param] = modified_series
        
        df_fault['fault_code'] = fault_code
        df_fault['fault_name'] = fault_info['name']
        all_data_dfs.append(df_fault)
    
    final_df = pd.concat(all_data_dfs, ignore_index=True)
    return final_df

class FaultClassifierPipeline:
    """å†…ç½®æ ‡å‡†åŒ–çš„åˆ†ç±»å™¨Pipeline"""
    
    def __init__(self, model, scaler=None):
        self.model = model
        self.scaler = scaler if scaler else StandardScaler()
        self.is_fitted = False
    
    def fit(self, X, y, **kwargs):
        X_scaled = self.scaler.fit_transform(X)
        self.model.fit(X_scaled, y, **kwargs)
        self.is_fitted = True
        return self
    
    def predict(self, X):
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)
    
    def predict_proba(self, X):
        X_scaled = self.scaler.transform(X)
        if hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X_scaled)
        return None

class FaultClassifierInference:
    """æ•…éšœåˆ†ç±»æ¨¡å‹æ¨ç†ç±»"""
    
    def __init__(self, model_path, metadata_path):
        self.model = joblib.load(model_path)
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        
        metadata = joblib.load(metadata_path)
        self.label_mapping = metadata['label_mapping']
        self.reverse_mapping = metadata['reverse_mapping']
        self.feature_names = metadata['feature_names']
        self.model_name = metadata['model_name']
        self.test_f1 = metadata.get('test_f1', 'N/A')
    
    def prepare_features(self, df):
        """å‡†å¤‡ç‰¹å¾"""
        exclude_cols = ['fault_code', 'fault_name', 'timestamp']
        sensor_cols = [col for col in df.columns 
                      if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]
        
        print(f"  ä½¿ç”¨åŸå§‹ä¼ æ„Ÿå™¨ç‰¹å¾: {len(sensor_cols)} ä¸ª")
        
        missing_features = set(self.feature_names) - set(sensor_cols)
        if missing_features:
            print(f"  è­¦å‘Š: ç¼ºå¤± {len(missing_features)} ä¸ªç‰¹å¾ï¼Œç”¨0å¡«å……")
            for feat in missing_features:
                df[feat] = 0
        
        X = df[self.feature_names].values
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        return X
    
    def predict_batch(self, samples, return_proba=False):
        """æ‰¹é‡é¢„æµ‹"""
        if isinstance(samples, pd.DataFrame):
            X = self.prepare_features(samples)
        else:
            X = np.array(samples)
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        y_pred = self.model.predict(X)
        fault_codes = [self.reverse_mapping[pred] for pred in y_pred]
        
        results = pd.DataFrame({
            'fault_code': fault_codes,
            'encoded_label': y_pred
        })
        
        if return_proba:
            try:
                proba = self.model.predict_proba(X)
                results['confidence'] = proba.max(axis=1)
                
                num_model_classes = proba.shape[1]
                for i in range(num_model_classes):
                    label = self.reverse_mapping.get(i, f'æœªçŸ¥ç±»åˆ«_{i}')
                    results[f'prob_{label}'] = proba[:, i]
            except Exception as e:
                print(f"  è­¦å‘Šï¼šæ— æ³•è·å–æ¦‚ç‡ã€‚é”™è¯¯: {e}")
                results['confidence'] = None
        
        return results

def generate_fault_report(df, predictions):
    """ç”Ÿæˆæ•…éšœåˆ†ææŠ¥å‘Š"""
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("æ•…éšœè¯Šæ–­åˆ†ææŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"æ ·æœ¬æ€»æ•°: {len(df)}")
    report_lines.append("")
    
    # ç»Ÿè®¡å„æ•…éšœç±»å‹
    fault_counts = predictions['fault_code'].value_counts().sort_index()
    
    report_lines.append("ã€æ•…éšœåˆ†å¸ƒç»Ÿè®¡ã€‘")
    report_lines.append("-" * 80)
    for fault_code, count in fault_counts.items():
        percentage = count / len(predictions) * 100
        fault_name = fault_point_analysis.get(fault_code, {}).get('name', f'æœªçŸ¥æ•…éšœ{fault_code}')
        report_lines.append(f"  æ•…éšœä»£ç  {fault_code} - {fault_name}: {count}æ¬¡ ({percentage:.2f}%)")
    report_lines.append("")
    
    # è¯¦ç»†æ•…éšœç‚¹åˆ†æ
    for fault_code in fault_counts.index:
        if fault_code not in fault_point_analysis:
            continue
            
        analysis = fault_point_analysis[fault_code]
        count = fault_counts[fault_code]
        percentage = count / len(predictions) * 100
        
        report_lines.append("=" * 80)
        report_lines.append(f"ã€æ•…éšœç±»å‹ {fault_code}ã€‘{analysis['name']}")
        report_lines.append("=" * 80)
        report_lines.append(f"æ£€å‡ºæ¬¡æ•°: {count}æ¬¡ ({percentage:.2f}%)")
        report_lines.append("")
        
        report_lines.append("â–º å…³é”®æ•…éšœç‚¹:")
        for i, point in enumerate(analysis['fault_points'], 1):
            report_lines.append(f"  {i}. {point}")
        report_lines.append("")
        
        report_lines.append("â–º å¯èƒ½åŸå› :")
        for i, cause in enumerate(analysis['root_causes'], 1):
            report_lines.append(f"  {i}. {cause}")
        report_lines.append("")
        
        report_lines.append("â–º å¤„ç†å»ºè®®:")
        for i, suggestion in enumerate(analysis['suggestions'], 1):
            report_lines.append(f"  {i}. {suggestion}")
        report_lines.append("")
    
    report_lines.append("=" * 80)
    report_lines.append("æŠ¥å‘Šç»“æŸ")
    report_lines.append("=" * 80)
    
    return "\n".join(report_lines)

def create_fault_distribution_chart(predictions):
    """åˆ›å»ºæ•…éšœåˆ†å¸ƒå›¾è¡¨"""
    fault_counts = predictions['fault_code'].value_counts().sort_index()
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # æŸ±çŠ¶å›¾
    fault_names = [fault_point_analysis.get(code, {}).get('name', f'æ•…éšœ{code}') 
                   for code in fault_counts.index]
    colors = plt.cm.Set3(range(len(fault_counts)))
    
    bars = ax1.bar(range(len(fault_counts)), fault_counts.values, color=colors, alpha=0.8, edgecolor='black')
    ax1.set_xticks(range(len(fault_counts)))
    ax1.set_xticklabels([f'{name}\n(ä»£ç {code})' for name, code in zip(fault_names, fault_counts.index)], 
                        rotation=45, ha='right')
    ax1.set_ylabel('æ£€å‡ºæ¬¡æ•°', fontsize=12, fontweight='bold')
    ax1.set_title('æ•…éšœç±»å‹æ£€å‡ºæ¬¡æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax1.grid(axis='y', alpha=0.3)
    
    for bar, count in zip(bars, fault_counts.values):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(count)}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # é¥¼å›¾
    wedges, texts, autotexts = ax2.pie(fault_counts.values, 
                                        labels=[f'{name}\n({count}æ¬¡)' for name, count in zip(fault_names, fault_counts.values)],
                                        autopct='%1.1f%%',
                                        colors=colors,
                                        explode=[0.05] * len(fault_counts),
                                        shadow=True,
                                        startangle=90)
    
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontsize(10)
        autotext.set_fontweight('bold')
    
    for text in texts:
        text.set_fontsize(9)
    
    ax2.set_title('æ•…éšœç±»å‹å æ¯”åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    
    plt.suptitle('æ•…éšœè¯Šæ–­ç»“æœåˆ†å¸ƒåˆ†æ', fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    
    buf = io.BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight', dpi=100)
    buf.seek(0)
    img = Image.open(buf)
    plt.close(fig)
    
    return img

classifier = None
model_options = {}
simulated_files = {}

def get_simulated_files():
    """è·å–æ‰€æœ‰æ¨¡æ‹Ÿç”Ÿæˆçš„æ–‡ä»¶"""
    global simulated_files
    simulated_files = {}
    
    if not DATA_DIR.exists():
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        return simulated_files
    
    sim_files = list(DATA_DIR.glob("simulated_fault_*.csv"))
    
    for file_path in sorted(sim_files, key=lambda x: x.stat().st_mtime, reverse=True):
        try:
            filename = file_path.stem
            parts = filename.split('_')
            if len(parts) >= 4:
                fault_code = int(parts[2])
                timestamp = '_'.join(parts[3:])
                
                fault_name = fault_definitions.get(fault_code, {}).get('name', f'æœªçŸ¥æ•…éšœ{fault_code}')
                
                try:
                    dt = datetime.strptime(timestamp, '%Y%m%d_%H%M%S')
                    time_str = dt.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    time_str = timestamp
                
                display_name = f"{fault_name} ({time_str})"
                simulated_files[display_name] = str(file_path)
        except Exception as e:
            logging.warning(f"è§£ææ–‡ä»¶åå¤±è´¥: {file_path.name}, é”™è¯¯: {e}")
            continue
    
    logging.info(f"æ‰¾åˆ° {len(simulated_files)} ä¸ªæ¨¡æ‹Ÿæ–‡ä»¶")
    return simulated_files

def initialize_models():
    """åˆå§‹åŒ–æ¨¡å‹é€‰é¡¹"""
    global model_options
    logging.info(f"å‘ç°æ¨¡å‹ç›®å½•: {MODEL_BASE_DIR}")
    if MODEL_BASE_DIR.exists():
        model_files = list(MODEL_BASE_DIR.glob("fault_model_*.pkl"))
        model_options = {f.stem: str(f) for f in model_files}
    logging.info(f"å‘ç° {len(model_options)} ä¸ªæ¨¡å‹æ–‡ä»¶")

def load_model(model_name):
    """åŠ è½½æŒ‡å®šæ¨¡å‹"""
    global classifier
    try:
        model_path = model_options[model_name]
        metadata_path = model_path.replace('fault_model_', 'model_metadata_')
        
        classifier = FaultClassifierInference(model_path, metadata_path)
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}\næ¨¡å‹ç±»å‹: {classifier.model_name}\nF1åˆ†æ•°: {classifier.test_f1}"
    except Exception as e:
        return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

def predict_from_csv(csv_file, model_name, simulated_file_name):
    """ä»CSVæ–‡ä»¶é¢„æµ‹ - ä¼˜å…ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®"""
    global classifier
    
    # ä¼˜å…ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    if simulated_file_name and simulated_file_name != "æš‚æ— æ¨¡æ‹Ÿæ•°æ®" and simulated_file_name in simulated_files:
        file_path = simulated_files[simulated_file_name]
        data_source = f"æ¨¡æ‹Ÿæ•°æ®: {simulated_file_name}"
        logging.info(f"ä½¿ç”¨æ¨¡æ‹Ÿæ–‡ä»¶: {file_path}")
    elif csv_file is not None:
        file_path = csv_file.name
        data_source = "ä¸Šä¼ æ–‡ä»¶"
        logging.info(f"ä½¿ç”¨ä¸Šä¼ æ–‡ä»¶: {file_path}")
    else:
        return None, "âŒ è¯·ä¸Šä¼ CSVæ–‡ä»¶æˆ–é€‰æ‹©æ¨¡æ‹Ÿæ•°æ®ï¼", None
    
    # æ£€æŸ¥æ¨¡å‹
    if not model_options or model_name == "æ— å¯ç”¨æ¨¡å‹" or model_name not in model_options:
        return None, "âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚", None
    
    if classifier is None or model_name not in model_options:
        status = load_model(model_name)
        if "å¤±è´¥" in status:
            return None, status, None
    
    try:
        df = pd.read_csv(file_path)
        logging.info(f"è¯»å–æ•°æ®: {len(df)} æ¡æ ·æœ¬")
        
        predictions = classifier.predict_batch(df, return_proba=True)
        output_df = pd.concat([df, predictions], axis=1)
        
        # ç”Ÿæˆæ•…éšœæŠ¥å‘Š
        report_text = generate_fault_report(df, predictions)
        
        # ç”Ÿæˆåˆ†å¸ƒå›¾è¡¨
        distribution_chart = create_fault_distribution_chart(predictions)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        output_path = DATA_DIR / f"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_df.to_csv(output_path, index=False)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = DATA_DIR / f"fault_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"æ•°æ®æ¥æº: {data_source}\n\n")
            f.write(report_text)
        
        result_summary = f"ğŸ“Š æ•…éšœè¯Šæ–­å®Œæˆ\n\n"
        result_summary += f"æ•°æ®æ¥æº: {data_source}\n"
        result_summary += f"æ ·æœ¬æ€»æ•°: {len(df)}\n"
        result_summary += f"é¢„æµ‹æ–‡ä»¶: {output_path}\n"
        result_summary += f"åˆ†ææŠ¥å‘Š: {report_path}\n"
        
        return result_summary, report_text, distribution_chart
        
    except Exception as e:
        error_msg = f"âŒ é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
        logging.error(error_msg, exc_info=True)
        return None, error_msg, None

def simulate_fault_from_db(fault_code, num_samples, pre_warning_severity, generate_pre_warning,
                          start_time, end_time, show_visualization):
    """ä»æ•°æ®åº“è·å–æ•°æ®å¹¶æ¨¡æ‹Ÿæ•…éšœ"""
    try:
        logging.info(f"ä»æ•°æ®åº“è·å–æ•°æ®: {start_time} åˆ° {end_time}")
        normal_df, error = fetch_normal_data_from_db(start_time, end_time)
        
        if error:
            return None, f"âŒ æ•°æ®è·å–å¤±è´¥: {error}", None
        
        if normal_df is None or len(normal_df) == 0:
            return None, "âŒ æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´", None
        
        logging.info(f"è·å–åˆ° {len(normal_df)} æ¡æ­£å¸¸æ•°æ®")
        
        available_samples = len(normal_df)
        if num_samples > available_samples:
            warning_msg = f"âš ï¸ è­¦å‘Šï¼šè¯·æ±‚ç”Ÿæˆ {num_samples} æ¡æ ·æœ¬ï¼Œä½†åªæœ‰ {available_samples} æ¡æ­£å¸¸æ•°æ®å¯ç”¨ã€‚\n"
            warning_msg += f"å°†è‡ªåŠ¨è°ƒæ•´ä¸ºç”Ÿæˆ {available_samples} æ¡æ ·æœ¬ã€‚\n\n"
            num_samples = available_samples
            logging.warning(f"æ ·æœ¬æ•°é‡å·²è°ƒæ•´ä¸º {num_samples}")
        else:
            warning_msg = ""
        
        if generate_pre_warning:
            num_pre_warning = int(num_samples * 0.3)
            num_fault = num_samples - num_pre_warning
            
            total_needed = num_pre_warning + num_fault
            if total_needed > available_samples:
                num_pre_warning = int(available_samples * 0.3)
                num_fault = available_samples - num_pre_warning
                logging.info(f"é‡æ–°åˆ†é…æ ·æœ¬: é¢„è­¦={num_pre_warning}, æ•…éšœ={num_fault}")
        else:
            num_pre_warning = 0
            num_fault = num_samples
        
        fault_df = generate_fault_data(
            normal_df=normal_df,
            fault_code=fault_code,
            fault_definitions=fault_definitions,
            num_pre_warning_samples=num_pre_warning,
            num_fault_samples=num_fault,
            pre_warning_severity=pre_warning_severity,
            generate_pre_warning=generate_pre_warning
        )
        
        result_text = warning_msg + f"ğŸ“Š æ•…éšœæ¨¡æ‹Ÿå®Œæˆï¼\n"
        result_text += f"æ•°æ®æ¥æº: æ•°æ®åº“ ({start_time} ~ {end_time})\n"
        result_text += f"åŸå§‹æ­£å¸¸æ•°æ®: {len(normal_df)} æ¡\n"
        result_text += f"ç”Ÿæˆæ•…éšœæ ·æœ¬: {len(fault_df)} æ¡\n\n"
        result_text += "ç”Ÿæˆæ•°æ®åˆ†å¸ƒ:\n"
        for code, count in fault_df['fault_code'].value_counts().items():
            fault_name = fault_df[fault_df['fault_code'] == code]['fault_name'].iloc[0]
            percentage = count / len(fault_df) * 100
            result_text += f"  {fault_name} (ä»£ç  {code}): {count} æ¡ ({percentage:.2f}%)\n"
        
        viz_img = None
        if show_visualization:
            exclude_cols = ['fault_code', 'fault_name', 'timestamp']
            feature_names = [col for col in fault_df.columns 
                           if col not in exclude_cols and fault_df[col].dtype in ['int64', 'float64']]
            feature_names = feature_names[:6]
            
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            fig.suptitle(f'æ•…éšœæ¨¡æ‹Ÿå¯è§†åŒ– - {fault_definitions[fault_code]["name"]}', 
                        fontsize=16, fontweight='bold')
            
            axes_flat = axes.flatten()
            
            for idx, col in enumerate(feature_names):
                if idx >= 6:
                    break
                ax = axes_flat[idx]
                
                for code in fault_df['fault_code'].unique():
                    mask = fault_df['fault_code'] == code
                    label = fault_df[mask]['fault_name'].iloc[0] if mask.any() else f'ä»£ç {code}'
                    ax.hist(fault_df.loc[mask, col], alpha=0.6, label=label, bins=20)
                
                ax.set_title(f'{col}', fontsize=10, fontweight='bold')
                ax.set_xlabel('å€¼')
                ax.set_ylabel('é¢‘æ•°')
                ax.legend(fontsize=8)
                ax.grid(True, alpha=0.3)
            
            for idx in range(len(feature_names), 6):
                axes_flat[idx].set_visible(False)
            
            plt.tight_layout()
            
            buf = io.BytesIO()
            plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
            buf.seek(0)
            viz_img = Image.open(buf)
            plt.close()
        
        output_path = DATA_DIR / f"simulated_fault_{fault_code}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        fault_df.to_csv(output_path, index=False)
        
        logging.info(f"âœ“ æ•…éšœæ•°æ®å·²ä¿å­˜: {output_path}")
        
        return str(output_path), result_text, viz_img
        
    except Exception as e:
        error_msg = f"âŒ æ•…éšœæ¨¡æ‹Ÿè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
        logging.error(error_msg, exc_info=True)
        return None, error_msg, None

def create_gradio_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    health_check_js = '''
    () => {
        let isConnected = true;
        setInterval(async () => {
            try {
                await fetch('/');
                if (!isConnected) {
                    console.log("æˆåŠŸé‡æ–°è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œæ­£åœ¨åˆ·æ–°é¡µé¢...");
                    location.reload();
                }
                isConnected = true;
            } catch (e) {
                if (isConnected) {
                    console.log("ä¸æœåŠ¡å™¨çš„è¿æ¥å·²æ–­å¼€ï¼Œç­‰å¾…é‡æ–°è¿æ¥...");
                }
                isConnected = false;
            }
        }, 2000);
    }
    '''
    
    now = datetime.now()
    today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
    default_start = today_start.strftime("%Y-%m-%d %H:%M:%S")
    default_end = now.strftime("%Y-%m-%d %H:%M:%S")
    
    with gr.Blocks(title="ç©ºå‹æœºæ™ºèƒ½æ•…éšœåˆ†ç±»è¯Šæ–­ç³»ç»Ÿ", js=health_check_js) as iface:
        gr.Markdown("""
        # ğŸ”§ ç©ºå‹æœºæ™ºèƒ½æ•…éšœåˆ†ç±»è¯Šæ–­ç³»ç»Ÿ
        **åŠŸèƒ½ç‰¹ç‚¹ï¼š** åŸºäºæœºå™¨å­¦ä¹ çš„è®¾å¤‡æ•…éšœåˆ†ç±»ä¸æ™ºèƒ½åˆ†æ
        """)
        
        with gr.Tab("ğŸ“Š æ•…éšœè¯Šæ–­"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ”§ æ¨¡å‹é…ç½®")
                    model_dropdown = gr.Dropdown(
                        choices=list(model_options.keys()) if model_options else ["æ— å¯ç”¨æ¨¡å‹"],
                        value=list(model_options.keys())[0] if model_options and "æ— å¯ç”¨æ¨¡å‹" not in model_options else "æ— å¯ç”¨æ¨¡å‹",
                        label="é€‰æ‹©æ¨¡å‹",
                        info="é€‰æ‹©ç”¨äºé¢„æµ‹çš„æ¨¡å‹"
                    )
                    
                    gr.Markdown("### ğŸ“ æ•°æ®è¾“å…¥ (ä¼˜å…ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®)")
                    
                    simulated_dropdown = gr.Dropdown(
                        choices=list(simulated_files.keys()) if simulated_files else ["æš‚æ— æ¨¡æ‹Ÿæ•°æ®"],
                        value=list(simulated_files.keys())[0] if simulated_files else "æš‚æ— æ¨¡æ‹Ÿæ•°æ®",
                        label="é€‰æ‹©æ¨¡æ‹Ÿæ•°æ®",
                        info="é€‰æ‹©ä¹‹å‰ç”Ÿæˆçš„æ¨¡æ‹Ÿæ•°æ®ï¼ˆä¼˜å…ˆï¼‰"
                    )
                    
                    refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡æ‹Ÿæ•°æ®åˆ—è¡¨", size="sm")
                    
                    gr.Markdown("æˆ–")
                    
                    csv_input = gr.File(
                        label="ä¸Šä¼ CSVæ–‡ä»¶ (å¤‡é€‰)",
                        file_types=[".csv"],
                        type="filepath"
                    )
                    
                    predict_btn = gr.Button("ğŸ” å¼€å§‹è¯Šæ–­", variant="primary", size="lg")
                    
                    with gr.Accordion("ğŸ“‹ ä½¿ç”¨è¯´æ˜", open=False):
                        gr.Markdown("""
                        **ä½¿ç”¨æ­¥éª¤ï¼š**
                        1. é€‰æ‹©å·²è®­ç»ƒçš„æ¨¡å‹
                        2. ä¼˜å…ˆé€‰æ‹©æ¨¡æ‹Ÿæ•°æ®ï¼ˆå¯ç‚¹å‡»åˆ·æ–°æŒ‰é’®æ›´æ–°åˆ—è¡¨ï¼‰
                        3. æˆ–ä¸Šä¼ CSVæ–‡ä»¶ä½œä¸ºå¤‡é€‰
                        4. ç‚¹å‡»"å¼€å§‹è¯Šæ–­"æŒ‰é’®
                        
                        **è¾“å‡ºå†…å®¹ï¼š**
                        - æ•…éšœåˆ†ç±»ç»“æœ
                        - è¯¦ç»†æ•…éšœç‚¹åˆ†ææŠ¥å‘Š
                        - å¯èƒ½åŸå› å’Œå¤„ç†å»ºè®®
                        - æ•…éšœåˆ†å¸ƒå¯è§†åŒ–
                        """)
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“Š è¯Šæ–­ç»“æœ")
                    result_summary = gr.Textbox(label="è¯Šæ–­æ¦‚è¦", lines=6, interactive=False)
                    
                    with gr.Row():
                        distribution_chart = gr.Image(label="æ•…éšœåˆ†å¸ƒåˆ†æ", height=400)
                    
                    fault_report = gr.Textbox(label="è¯¦ç»†æ•…éšœåˆ†ææŠ¥å‘Š", lines=25, interactive=False)
        
        with gr.Tab("ğŸ¯ æ•…éšœæ¨¡æ‹Ÿ"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“… æ—¶é—´èŒƒå›´")
                    with gr.Row():
                        start_time_input = gr.Textbox(
                            label="å¼€å§‹æ—¶é—´",
                            value=default_start,
                            placeholder="2025-11-12 00:00:00",
                            info="æ ¼å¼: YYYY-MM-DD HH:MM:SS"
                        )
                        end_time_input = gr.Textbox(
                            label="ç»“æŸæ—¶é—´",
                            value=default_end,
                            placeholder="2025-11-12 23:59:59",
                            info="æ ¼å¼: YYYY-MM-DD HH:MM:SS"
                        )
                    
                    with gr.Row():
                        today_btn = gr.Button("ğŸ“… ä»Šå¤©", size="sm")
                        yesterday_btn = gr.Button("ğŸ“… æ˜¨å¤©", size="sm")
                        last_week_btn = gr.Button("ğŸ“… æœ€è¿‘7å¤©", size="sm")
                    
                    gr.Markdown("### âš™ï¸ æ•…éšœå‚æ•°")
                    fault_code_input = gr.Dropdown(
                        choices=[(f"{k}: {v['name']}", k) for k, v in fault_definitions.items()],
                        value=list(fault_definitions.keys())[0] if fault_definitions else None,
                        label="æ•…éšœç±»å‹",
                        info="é€‰æ‹©è¦æ¨¡æ‹Ÿçš„æ•…éšœç±»å‹"
                    )
                    
                    num_samples_slider = gr.Slider(
                        minimum=2,
                        maximum=500,
                        value=200,
                        step=50,
                        label="ç”Ÿæˆæ ·æœ¬æ•°"
                    )
                    
                    severity_slider = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=0.5,
                        step=0.1,
                        label="é¢„è­¦ä¸¥é‡ç¨‹åº¦",
                        info="0.1=è½»å¾®, 1.0=ä¸¥é‡"
                    )
                    
                    gen_warning_checkbox = gr.Checkbox(
                        label="ç”Ÿæˆé¢„è­¦æ•°æ®",
                        value=True
                    )
                    
                    show_sim_viz_checkbox = gr.Checkbox(
                        label="æ˜¾ç¤ºå¯è§†åŒ–ç»“æœ",
                        value=True
                    )
                    
                    simulate_btn = gr.Button("ğŸ¯ å¼€å§‹æ¨¡æ‹Ÿ", variant="primary", size="lg")
                    
                    with gr.Accordion("ğŸ“‹ ä½¿ç”¨è¯´æ˜", open=False):
                        gr.Markdown("""
                        **ä½¿ç”¨æ­¥éª¤ï¼š**
                        1. è®¾ç½®æ—¶é—´èŒƒå›´ï¼ˆé»˜è®¤ä¸ºä»Šå¤©é›¶ç‚¹åˆ°å½“å‰æ—¶é—´ï¼‰
                        2. é€‰æ‹©æ•…éšœç±»å‹å’Œå‚æ•°
                        3. ç‚¹å‡»"å¼€å§‹æ¨¡æ‹Ÿ"æŒ‰é’®
                        
                        **æ•°æ®æ¥æºï¼š**
                        - ä»TDengineæ•°æ®åº“å®æ—¶è·å–æ­£å¸¸è¿è¡Œæ•°æ®
                        - åŸºäºçœŸå®æ•°æ®ç”Ÿæˆæ¨¡æ‹Ÿæ•…éšœæ ·æœ¬
                        - ç”Ÿæˆçš„æ•°æ®å¯ç”¨äºæ•…éšœè¯Šæ–­æµ‹è¯•
                        """)
                
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“Š æ¨¡æ‹Ÿç»“æœ")
                    sim_result_file = gr.Textbox(label="ç»“æœæ–‡ä»¶è·¯å¾„", interactive=False)
                    sim_result_text = gr.Textbox(label="æ¨¡æ‹Ÿç»Ÿè®¡", lines=10, interactive=False)
                    
                    sim_viz_output = gr.Image(label="ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–", height=500)
        
        # åˆ·æ–°æ¨¡æ‹Ÿæ•°æ®åˆ—è¡¨
        def refresh_simulated_files():
            get_simulated_files()
            choices = list(simulated_files.keys()) if simulated_files else ["æš‚æ— æ¨¡æ‹Ÿæ•°æ®"]
            value = choices[0] if simulated_files else "æš‚æ— æ¨¡æ‹Ÿæ•°æ®"
            return gr.update(choices=choices, value=value)
        
        refresh_btn.click(
            refresh_simulated_files,
            outputs=[simulated_dropdown]
        )
        
        # å¿«æ·æ—¶é—´æŒ‰é’®
        def set_today():
            now = datetime.now()
            today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
            return today_start.strftime("%Y-%m-%d %H:%M:%S"), now.strftime("%Y-%m-%d %H:%M:%S")
        
        def set_yesterday():
            now = datetime.now()
            yesterday = now - timedelta(days=1)
            yesterday_start = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
            yesterday_end = yesterday.replace(hour=23, minute=59, second=59, microsecond=0)
            return yesterday_start.strftime("%Y-%m-%d %H:%M:%S"), yesterday_end.strftime("%Y-%m-%d %H:%M:%S")
        
        def set_last_week():
            now = datetime.now()
            week_ago = now - timedelta(days=7)
            return week_ago.strftime("%Y-%m-%d %H:%M:%S"), now.strftime("%Y-%m-%d %H:%M:%S")
        
        today_btn.click(set_today, outputs=[start_time_input, end_time_input])
        yesterday_btn.click(set_yesterday, outputs=[start_time_input, end_time_input])
        last_week_btn.click(set_last_week, outputs=[start_time_input, end_time_input])
        
        # é¢„æµ‹æŒ‰é’®äº‹ä»¶
        predict_btn.click(
            predict_from_csv,
            inputs=[csv_input, model_dropdown, simulated_dropdown],
            outputs=[result_summary, fault_report, distribution_chart]
        )
        
        # æ¨¡æ‹ŸæŒ‰é’®äº‹ä»¶
        simulate_btn.click(
            simulate_fault_from_db,
            inputs=[
                fault_code_input, num_samples_slider, severity_slider,
                gen_warning_checkbox, start_time_input, end_time_input,
                show_sim_viz_checkbox
            ],
            outputs=[sim_result_file, sim_result_text, sim_viz_output]
        )
    
    return iface

def main():
    """ä¸»å‡½æ•°"""
    print(f"\n{'='*80}")
    print("æ•…éšœåˆ†ç±»æ¨ç†ç³»ç»Ÿ Gradio åº”ç”¨ (ä¼˜åŒ–ç‰ˆ)")
    print(f"{'='*80}\n")
    
    initialize_models()
    get_simulated_files()
    
    monitor_manager = None
    if MultiDirectoryMonitor is not None:
        monitor_manager = MultiDirectoryMonitor(restart_signal_file_name=RESTART_SIGNAL_FILENAME)
        monitor_manager.add_directory(MODEL_BASE_DIR)
        # monitor_manager.add_directory(DATA_DIR)
        if EXAMPLE_DIR.exists():
            monitor_manager.add_directory(EXAMPLE_DIR)
        
        if not monitor_manager.start_all():
            logging.error("âŒ å¯åŠ¨ç›®å½•ç›‘æ§å¤±è´¥")
        else:
            logging.info("âœ… ç›®å½•ç›‘æ§å·²å¯åŠ¨")
    
    port = 7864
    if len(sys.argv) > 1:
        try:
            port = int(sys.argv[1])
            if port < 1024 or port > 65535:
                logging.warning(f"ç«¯å£å· {port} ä¸åœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼Œä½¿ç”¨é»˜è®¤ç«¯å£ 7864")
                port = 7864
        except ValueError:
            logging.warning(f"æ— æ•ˆçš„ç«¯å£å·å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤ç«¯å£ 7864")
    
    iface = create_gradio_interface()
    
    try:
        iface.launch(
            server_name="0.0.0.0",
            server_port=port,
            share=False
        )
    finally:
        if monitor_manager is not None:
            monitor_manager.stop_all(join_threads=True)
            logging.info("ç›®å½•ç›‘æ§å·²åœæ­¢")

if __name__ == '__main__':
    main()