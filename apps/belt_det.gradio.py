import gradio as gr
import paddlex as pdx
import os
import sys
import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from pathlib import Path
import logging
import subprocess
from utils.app_utils import AppUtils as util
from utils.app_utils import MultiDirectoryMonitor

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- å…¨å±€æ£€æµ‹å®ä¾‹ ---
detector_instance = None

# --- ç›®å½•é…ç½® ---
BASE_DIR = Path(__file__).parent.parent
MODEL_BASE_DIR = BASE_DIR / "model" / "belt_det" / "model"
RESTART_SIGNAL_FILENAME = ".restart_signal_belt_det"
EXAMPLE_DIR = BASE_DIR / "dataset" / "belt_det"
OUTPUT_DIR = BASE_DIR / "model" / "belt_det" / "output"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# æ¨¡å‹é€‰é¡¹
model_options = util.generate_paddlex_model_options(MODEL_BASE_DIR)


class VideoDetector:
    """è§†é¢‘/å›¾ç‰‡ç›®æ ‡æ£€æµ‹å™¨ç±»"""
    
    def __init__(self, model_dir, threshold=0.3):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        self.model_dir = model_dir
        self.threshold = threshold
        self.predictor = None
        self.class_names = []
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨ç†æ¨¡å‹"""
        try:
            logging.info(f"Loading model from {self.model_dir}...")
            model_path = Path(self.model_dir)
            model_name = model_path.name
            self.predictor = pdx.create_model(
                model_name=model_name, 
                model_dir=self.model_dir
            )
            
            if hasattr(self.predictor, 'labels'):
                self.class_names = self.predictor.labels
                logging.info(f"Model classes: {self.class_names}")
            
            logging.info("Model loaded successfully!")
            
        except Exception as e:
            logging.error(f"Error loading model: {e}")
            raise
    
    def detect_image(self, image_path):
        """æ£€æµ‹å•å¼ å›¾ç‰‡"""
        frame = cv2.imread(image_path)
        if frame is None:
            raise ValueError(f"Cannot read image from {image_path}")
        
        vis_frame, num_detections, detection_info = self._detect_frame(frame)
        return vis_frame, num_detections, detection_info
    
    def detect_video(self, video_path, progress=gr.Progress()):
        """æ£€æµ‹è§†é¢‘"""
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")
        
        # è·å–è§†é¢‘å±æ€§
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        logging.info(f"Video: {width}x{height} @ {fps}FPS, {total_frames} frames")
        
        temp_output = OUTPUT_DIR / f"temp_{Path(video_path).stem}.mp4"
        final_output = OUTPUT_DIR / f"detected_{Path(video_path).stem}.mp4"
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v') 
        writer = cv2.VideoWriter(str(temp_output), fourcc, fps, (width, height))
        
        if not writer.isOpened():
            cap.release()
            raise ValueError("Cannot initialize video writer")
        
        frame_count = 0
        total_detections = 0
        
        try:
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1

                vis_frame, num_detections, _ = self._detect_frame(frame)
                total_detections += num_detections
                
                cv2.putText(vis_frame, f"Frame: {frame_count}/{total_frames}", 
                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(vis_frame, f"Detections: {num_detections}", 
                        (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                
                writer.write(vis_frame)
                
                if total_frames > 0:
                    progress_value = frame_count / total_frames
                    progress_text = f"å¤„ç†ä¸­ {frame_count}/{total_frames}"
                    progress(progress_value, desc=progress_text)
        
        finally:
            cap.release()
            writer.release()
        
        # è½¬æ¢ä¸ºH.264æ ¼å¼
        if temp_output.exists():
            try:
                cmd = [
                    'ffmpeg', '-y', '-i', str(temp_output),
                    '-c:v', 'libx264', '-preset', 'medium',
                    '-crf', '23', '-pix_fmt', 'yuv420p',
                    str(final_output)
                ]
                subprocess.run(cmd, capture_output=True, check=True)
                temp_output.unlink()
                logging.info(f"Video saved: {final_output}")
            except subprocess.CalledProcessError as e:
                error_message = e.stderr.decode() if e.stderr else str(e)
                logging.error(f"FFmpeg è½¬æ¢å¤±è´¥ï¼Œè¯¦ç»†åŸå› : \n{error_message}")
                final_output = temp_output
            except Exception as e:
                logging.error(f"è§†é¢‘å¤„ç†æœªçŸ¥é”™è¯¯: {e}")
                final_output = temp_output
        
        avg_det = total_detections / frame_count if frame_count > 0 else 0
        status = f"å¤„ç†å®Œæˆï¼æ€»å¸§æ•°: {frame_count}, æ€»æ£€æµ‹æ•°: {total_detections}, å¹³å‡æ£€æµ‹/å¸§: {avg_det:.2f}"
        
        return str(final_output), status
    
    def _detect_frame(self, frame):
        """å¯¹å•å¸§è¿›è¡Œæ£€æµ‹"""
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        try:
            result_generator = self.predictor.predict(img_rgb)
            result = next(result_generator)
            vis_frame, num_detections, detection_info = self._draw_detections(frame, result)
        except Exception as e:
            logging.error(f"Prediction error: {e}")
            return frame, 0, []
        
        return vis_frame, num_detections, detection_info
    
    def _draw_detections(self, frame, result):
        """ç»˜åˆ¶æ£€æµ‹ç»“æœ"""
        vis_frame = frame.copy()
        
        if isinstance(result, dict):
            boxes = result.get('boxes', [])
        elif hasattr(result, 'boxes'):
            boxes = result.boxes
        else:
            return vis_frame, 0, []
        
        if not boxes or len(boxes) == 0:
            return vis_frame, 0, []
        
        num_detections = 0
        detection_info = []
        
        # è½¬æ¢ä¸ºPILæ ¼å¼ä»¥æ”¯æŒä¸­æ–‡ - æ”¾åœ¨æœ€å¼€å§‹
        pil_img = Image.fromarray(cv2.cvtColor(vis_frame, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_img)
        
        # åŠ è½½ä¸­æ–‡å­—ä½“
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc", 24)
        except:
            font = ImageFont.load_default()
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†
        for box_dict in boxes:
            try:
                score = float(box_dict['score'])
                label = box_dict['label']
                cls_id = int(box_dict['cls_id'])
                coordinate = box_dict['coordinate']
                
                if score < self.threshold:
                    continue
                
                num_detections += 1
                x1, y1, x2, y2 = map(int, coordinate)
                
                # ç¡®ä¿åæ ‡åœ¨èŒƒå›´å†…
                h, w = frame.shape[:2]
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w, x2), min(h, y2)
                
                color = self._get_color(cls_id)
                
                # ä½¿ç”¨ ImageDraw ç»˜åˆ¶çŸ©å½¢æ¡†ï¼ˆè€Œä¸æ˜¯ cv2.rectangleï¼‰
                draw.rectangle([(x1, y1), (x2, y2)], outline=color, width=3)
                
                # å‡†å¤‡æ–‡æœ¬
                text = f"{label}: {score:.2f}"
                detection_info.append(f"{label} ({score:.2%})")
                
                # è·å–æ–‡æœ¬å°ºå¯¸
                bbox = draw.textbbox((0, 0), text, font=font)
                text_w = bbox[2] - bbox[0]
                text_h = bbox[3] - bbox[1]
                
                # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
                draw.rectangle([(x1, y1 - text_h - 10), (x1 + text_w + 10, y1)], 
                            fill=color)
                
                # ç»˜åˆ¶æ–‡æœ¬
                draw.text((x1 + 5, y1 - text_h - 5), text, 
                        fill=(255, 255, 255), font=font)
                
            except Exception as e:
                logging.error(f"Error drawing box: {e}")
                continue
        
        # è½¬æ¢å›OpenCVæ ¼å¼
        vis_frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
        
        return vis_frame, num_detections, detection_info
    
    def _get_color(self, label):
        """æ ¹æ®æ ‡ç­¾è·å–é¢œè‰²"""
        colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255),
            (255, 255, 0), (255, 0, 255), (0, 255, 255),
            (128, 0, 0), (0, 128, 0), (0, 0, 128),
        ]
        return colors[int(label) % len(colors)]


def initialize_detector(model_choice):
    """åˆå§‹åŒ–æ£€æµ‹å™¨"""
    global detector_instance
    try:
        models_config = model_options[model_choice]
        detector_instance = VideoDetector(
            model_dir=models_config,
            threshold=0.3
        )
        return "âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ"
    except Exception as e:
        logging.error(f"åˆå§‹åŒ–æ¨¡å‹å¤±è´¥: {e}")
        return f"âœ— åˆå§‹åŒ–å¤±è´¥: {str(e)}"


def detect_image(image_path, model_choice, threshold):
    """æ£€æµ‹å›¾ç‰‡"""
    global detector_instance
    
    if image_path is None:
        return None, "è¯·å…ˆä¸Šä¼ æˆ–é€‰æ‹©å›¾ç‰‡"
    
    if detector_instance is None:
        initialize_detector(model_choice)
    
    # æ›´æ–°é˜ˆå€¼
    detector_instance.threshold = threshold
    
    try:
        vis_frame, num_detections, detection_info = detector_instance.detect_image(image_path)
        
        # è½¬æ¢ä¸ºRGBæ ¼å¼
        result_image = cv2.cvtColor(vis_frame, cv2.COLOR_BGR2RGB)
        
        status = f"æ£€æµ‹åˆ° {num_detections} ä¸ªç›®æ ‡"
        if detection_info:
            status += f"\nç›®æ ‡è¯¦æƒ…: {', '.join(detection_info)}"
        
        return result_image, status
        
    except Exception as e:
        logging.error(f"æ£€æµ‹å‡ºé”™: {e}")
        return None, f"æ£€æµ‹å‡ºé”™: {str(e)}"


def detect_video(video_path, model_choice, threshold, progress=gr.Progress()):
    """æ£€æµ‹è§†é¢‘"""
    global detector_instance
    
    if video_path is None:
        return None, "è¯·å…ˆä¸Šä¼ è§†é¢‘"
    
    if detector_instance is None:
        initialize_detector(model_choice)
    
    # æ›´æ–°é˜ˆå€¼
    detector_instance.threshold = threshold
    
    try:
        output_path, status = detector_instance.detect_video(video_path, progress)
        return output_path, status
        
    except Exception as e:
        logging.error(f"è§†é¢‘æ£€æµ‹å‡ºé”™: {e}")
        return None, f"è§†é¢‘æ£€æµ‹å‡ºé”™: {str(e)}"


def change_model(model_choice):
    """åˆ‡æ¢æ¨¡å‹"""
    global detector_instance
    detector_instance = None
    return f"å·²é€‰æ‹©æ¨¡å‹: {model_choice}ï¼Œä¸‹æ¬¡æ£€æµ‹æ—¶å°†è‡ªåŠ¨åŠ è½½"


def clear_outputs():
    """æ¸…ç©ºè¾“å‡º"""
    return None, None, "", None, ""


def create_gradio_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    health_check_js = '''
    () => {
        let isConnected = true;
        setInterval(async () => {
            try {
                await fetch('/');
                if (!isConnected) {
                    console.log("é‡æ–°è¿æ¥æˆåŠŸï¼Œåˆ·æ–°é¡µé¢...");
                    location.reload();
                }
                isConnected = true;
            } catch (e) {
                if (isConnected) {
                    console.log("è¿æ¥æ–­å¼€ï¼Œç­‰å¾…é‡è¿...");
                }
                isConnected = false;
            }
        }, 2000);
    }
    '''
    
    with gr.Blocks(title="ä¼ é€å¸¦ç›®æ ‡æ£€æµ‹", theme=gr.themes.Default(), js=health_check_js) as iface:
        gr.Markdown("""
        # ğŸšš ä¼ é€å¸¦ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ
        **åŠŸèƒ½ç‰¹ç‚¹ï¼š** åŸºäºPaddleX PP-YOLOE+æ¨¡å‹ã€æ”¯æŒå›¾ç‰‡å’Œè§†é¢‘æ£€æµ‹ã€å®æ—¶å¯è§†åŒ–æ ‡æ³¨ã€å¯è°ƒèŠ‚æ£€æµ‹é˜ˆå€¼
        """)
        
        with gr.Row():
            # å·¦ä¾§ï¼šç¤ºä¾‹å’Œæ¨¡å‹é€‰æ‹©
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ–¼ï¸ ç¤ºä¾‹å›¾ç‰‡")
                example_gallery = gr.Gallery(
                    value=util.get_current_examples(EXAMPLE_DIR),
                    label="ç‚¹å‡»é€‰æ‹©ç¤ºä¾‹",
                    show_label=False,
                    columns=4,
                    rows=1,
                    height=200,
                    allow_preview=False
                )
                
                gr.Markdown("### âš™ï¸ æ¨¡å‹é…ç½®")
                model_selector = gr.Dropdown(
                    choices=list(model_options.keys()),
                    value=list(model_options.keys())[0] if model_options else None,
                    label="é€‰æ‹©æ£€æµ‹æ¨¡å‹",
                    info="æ”¯æŒPaddleXé¢„è®­ç»ƒå’Œè‡ªå®šä¹‰æ¨¡å‹"
                )
                
                threshold_slider = gr.Slider(
                    minimum=0.1,
                    maximum=0.9,
                    value=0.3,
                    step=0.05,
                    label="æ£€æµ‹é˜ˆå€¼",
                    info="é™ä½é˜ˆå€¼å¯æé«˜å¬å›ç‡"
                )
                
                with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=True):
                    gr.Markdown("""
                    **æ“ä½œæ­¥éª¤ï¼š**
                    1. é€‰æ‹©æ£€æµ‹æ¨¡å‹å’Œé˜ˆå€¼
                    2. ä¸Šä¼ å›¾ç‰‡æˆ–é€‰æ‹©ç¤ºä¾‹
                    3. ç‚¹å‡»å¯¹åº”çš„æ£€æµ‹æŒ‰é’®
                    4. æŸ¥çœ‹æ£€æµ‹ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
                    
                    **å›¾ç‰‡æ ¼å¼ï¼š** JPG, PNG, JPEG
                    **è§†é¢‘æ ¼å¼ï¼š** MP4, AVI, MOV
                    
                    **æ¨¡å‹ï¼š** PP-YOLOE+ (PaddleX)
                    """)
            
            # å³ä¾§ï¼šä¸Šä¼ å’Œç»“æœ
            with gr.Column(scale=1):
                with gr.Tabs():
                    # å›¾ç‰‡æ£€æµ‹æ ‡ç­¾é¡µ
                    with gr.Tab("ğŸ“· å›¾ç‰‡æ£€æµ‹"):
                        gr.Markdown("### ğŸ“¤ ä¸Šä¼ å›¾ç‰‡")
                        input_image = gr.Image(
                            type="filepath",
                            label="ä¸Šä¼ å›¾ç‰‡",
                            height=200,
                            sources=['upload']
                        )
                        
                        with gr.Row():
                            image_detect_btn = gr.Button("ğŸ” æ£€æµ‹å›¾ç‰‡", variant="primary", size="lg")
                            image_clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
                        
                        gr.Markdown("### ğŸ“‹ æ£€æµ‹ç»“æœ")
                        output_image = gr.Image(label="æ£€æµ‹ç»“æœ", height=500)
                        image_status = gr.Textbox(label="æ£€æµ‹ä¿¡æ¯", lines=3, interactive=False)
                    
                    # è§†é¢‘æ£€æµ‹æ ‡ç­¾é¡µ
                    with gr.Tab("ğŸ¬ è§†é¢‘æ£€æµ‹",visible=False):
                        gr.Markdown("### ğŸ“¤ ä¸Šä¼ è§†é¢‘")
                        input_video = gr.Video(
                            label="ä¸Šä¼ è§†é¢‘",
                            height=200
                        )
                        
                        with gr.Row():
                            video_detect_btn = gr.Button("ğŸ” æ£€æµ‹è§†é¢‘", variant="primary", size="lg")
                            video_clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
                        
                        gr.Markdown("### ğŸ“‹ æ£€æµ‹ç»“æœ")
                        output_video = gr.Video(label="æ£€æµ‹ç»“æœè§†é¢‘", height=500)
                        video_status = gr.Textbox(label="å¤„ç†ä¿¡æ¯", lines=3, interactive=False)
        
        # äº‹ä»¶ç»‘å®š
        def select_example(evt: gr.SelectData):
            current_examples = util.get_current_examples(EXAMPLE_DIR)
            if current_examples and evt.index < len(current_examples):
                return current_examples[evt.index]
            return None
        
        example_gallery.select(select_example, None, outputs=[input_image])
        
        image_detect_btn.click(
            fn=detect_image,
            inputs=[input_image, model_selector, threshold_slider],
            outputs=[output_image, image_status]
        )
        
        image_clear_btn.click(
            fn=lambda: (None, None, ""),
            outputs=[input_image, output_image, image_status]
        )
        
        video_detect_btn.click(
            fn=detect_video,
            inputs=[input_video, model_selector, threshold_slider],
            outputs=[output_video, video_status]
        )
        
        video_clear_btn.click(
            fn=lambda: (None, None, ""),
            outputs=[input_video, output_video, video_status]
        )
        
        model_selector.change(
            fn=change_model,
            inputs=[model_selector]
        )
        
        return iface


def main():
    """ä¸»å‡½æ•°"""
    monitor_manager = MultiDirectoryMonitor(restart_signal_file_name=RESTART_SIGNAL_FILENAME)
    monitor_manager.add_directory(MODEL_BASE_DIR)
    monitor_manager.add_directory(EXAMPLE_DIR)
    
    if not monitor_manager.start_all():
        logging.error("âŒ å¯åŠ¨ç›®å½•ç›‘æ§å¤±è´¥")
        return
    
    port = 7862
    if len(sys.argv) > 1:
        try:
            port = int(sys.argv[1])
            if port < 1024 or port > 65535:
                logging.warning(f"ç«¯å£å· {port} ä¸åœ¨æœ‰æ•ˆèŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤ç«¯å£ 7862")
                port = 7862
        except ValueError:
            logging.warning(f"æ— æ•ˆç«¯å£å·ï¼Œä½¿ç”¨é»˜è®¤ç«¯å£ 7862")
    
    iface = create_gradio_interface()
    
    try:
        iface.launch(
            server_name="0.0.0.0",
            server_port=port,
            share=False,
        )
    finally:
        monitor_manager.stop_all(join_threads=True)


if __name__ == "__main__":
    main()